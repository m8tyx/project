{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e4c7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from pathlib import Path\n",
    "from math import sqrt\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2afdb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesRequestPredictor30MinHybrid:\n",
    "    def __init__(self, sequence_length=144, prediction_horizon=48, rare_threshold=50):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.rare_threshold = rare_threshold\n",
    "        self.scaler = StandardScaler()\n",
    "        self.geo_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.rf_models = {}\n",
    "        self.slot_counts = {}\n",
    "        self.slot_means = {}\n",
    "        self.global_slot_means = {}\n",
    "\n",
    "    # ======================\n",
    "    # Preprocess\n",
    "    # ======================\n",
    "    def load_and_preprocess(self, df):\n",
    "        if df.empty:\n",
    "            for col in [\"request_date\",\"origin_geo_hash\",\"time_slot\",\"request_count\"]:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "            return df\n",
    "\n",
    "        df[\"request_date\"] = pd.to_datetime(df[\"request_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "        # Extract slot\n",
    "        def extract_slot(val):\n",
    "            if pd.isna(val):\n",
    "                return -1\n",
    "            if isinstance(val, str) and \"-\" in val:\n",
    "                try:\n",
    "                    h, m = map(int, val.split(\"-\")[0].strip().split(\":\"))\n",
    "                    return h*2 + m//30\n",
    "                except:\n",
    "                    return -1\n",
    "            return -1\n",
    "        df[\"slot_30min\"] = df[\"time_slot\"].apply(extract_slot)\n",
    "        df[\"is_missing_slot\"] = (df[\"slot_30min\"]==-1).astype(int)\n",
    "\n",
    "        # Encode geo\n",
    "        if df[\"origin_geo_hash\"].notna().sum() > 0:\n",
    "            df[\"geo_encoded\"] = self.geo_encoder.fit_transform(df[\"origin_geo_hash\"])\n",
    "        else:\n",
    "            df[\"geo_encoded\"] = 0\n",
    "\n",
    "        df[\"day_of_week\"] = df[\"request_date\"].dt.dayofweek.fillna(0).astype(int)\n",
    "        df[\"is_weekend\"] = (df[\"day_of_week\"] >=5).astype(int)\n",
    "\n",
    "        df = df.sort_values([\"origin_geo_hash\",\"request_date\",\"slot_30min\"])\n",
    "        return df\n",
    "\n",
    "    # ======================\n",
    "    # Training\n",
    "    # ======================\n",
    "    def train(self, df=None, epochs=20, validation_split=0.2):\n",
    "        if df is None or df.empty:\n",
    "            print(\"‚ö†Ô∏è Tidak ada data untuk train.\")\n",
    "            return df, None\n",
    "\n",
    "        # Hitung slot counts & mean historis\n",
    "        counts = df.groupby([\"origin_geo_hash\",\"slot_30min\"]).size()\n",
    "        self.slot_counts = counts.to_dict()\n",
    "        self.slot_means = df.groupby([\"origin_geo_hash\",\"slot_30min\"])[\"request_count\"].mean().to_dict()\n",
    "        self.global_slot_means = df.groupby(\"slot_30min\")[\"request_count\"].mean().to_dict()\n",
    "\n",
    "        # Buat sequences\n",
    "        X, y = self.create_sequences(df)\n",
    "        if X.size != 0:\n",
    "            # Split train/val\n",
    "            split_index = int(len(X)*(1-validation_split))\n",
    "            X_train, X_val = X[:split_index], X[split_index:]\n",
    "            y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "            # Scale hanya di train\n",
    "            self.scaler.fit(X_train.reshape(-1, X_train.shape[-1]))\n",
    "            X_train_scaled = self.scaler.transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "            X_val_scaled   = self.scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "\n",
    "            # Build LSTM\n",
    "            self.model = self.build_model((X.shape[1], X.shape[2]))\n",
    "            callbacks = [\n",
    "                EarlyStopping(patience=10, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "            ]\n",
    "            self.model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val),\n",
    "                           epochs=epochs, batch_size=32, callbacks=callbacks, verbose=1)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Tidak cukup data untuk LSTM.\")\n",
    "\n",
    "        # Random Forest untuk rare slots\n",
    "        for (geo, slot), cnt in self.slot_counts.items():\n",
    "            if cnt < self.rare_threshold:\n",
    "                slot_df = df[(df[\"origin_geo_hash\"]==geo) & (df[\"slot_30min\"]==slot)]\n",
    "                if len(slot_df) >= 3:\n",
    "                    X_rf = slot_df[[\"day_of_week\",\"is_weekend\",\"geo_encoded\"]].fillna(0)\n",
    "                    y_rf = slot_df[\"request_count\"].fillna(0)\n",
    "                    rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "                    try:\n",
    "                        rf.fit(X_rf, y_rf)\n",
    "                        self.rf_models[(geo, slot)] = rf\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è RF fit error geo={geo}, slot={slot}: {e}\")\n",
    "\n",
    "        print(f\"üîπ RF models trained: {len(self.rf_models)}\")\n",
    "        return df, None\n",
    "\n",
    "    # ======================\n",
    "    # Sequence generator\n",
    "    # ======================\n",
    "    def create_sequences(self, data):\n",
    "        if data.empty:\n",
    "            return np.array([]), np.array([])\n",
    "        sequences, targets = [], []\n",
    "        features = [\"request_count\",\"slot_30min\",\"day_of_week\",\"is_weekend\",\"geo_encoded\"]\n",
    "        for geo in data[\"origin_geo_hash\"].unique():\n",
    "            geo_data = data[data[\"origin_geo_hash\"]==geo].sort_values([\"request_date\",\"slot_30min\"])\n",
    "            feature_data = geo_data[features].fillna(0).values.copy()\n",
    "            feature_data[:,0] = np.log1p(feature_data[:,0] + 1e-6)  # log-transform count\n",
    "            for i in range(len(feature_data)-self.sequence_length-self.prediction_horizon+1):\n",
    "                seq = feature_data[i:i+self.sequence_length]\n",
    "                target = feature_data[i+self.sequence_length:i+self.sequence_length+self.prediction_horizon,0]\n",
    "                sequences.append(seq)\n",
    "                targets.append(target)\n",
    "        if len(sequences)==0:\n",
    "            return np.array([]), np.array([])\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    # ======================\n",
    "    # Build LSTM\n",
    "    # ======================\n",
    "    def build_model(self, input_shape):\n",
    "        model = Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            LSTM(128, return_sequences=True, recurrent_dropout=0.2),\n",
    "            LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(self.prediction_horizon, activation=\"linear\")\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "        return model\n",
    "\n",
    "    # ======================\n",
    "    # Prediction\n",
    "    # ======================\n",
    "    def predict_all_next_day_30min_filtered(self, df_last_day):\n",
    "        if df_last_day.empty:\n",
    "            return pd.DataFrame(columns=[\"geo_hash\",\"date\",\"slot_30min\",\"predicted_request_count\"])\n",
    "\n",
    "        results = []\n",
    "        for geo in df_last_day[\"origin_geo_hash\"].unique():\n",
    "            geo_data = df_last_day[df_last_day[\"origin_geo_hash\"]==geo].sort_values(\"slot_30min\")\n",
    "            last_date = geo_data[\"request_date\"].max()\n",
    "            tomorrow_date = last_date + timedelta(days=1)\n",
    "            all_slots = np.arange(48)\n",
    "\n",
    "            out = {\"geo_hash\":[],\"date\":[],\"slot_30min\":[],\"predicted_request_count\":[]}\n",
    "            for slot in all_slots:\n",
    "                key = (geo,int(slot))\n",
    "                if key in self.rf_models:   # RF model khusus\n",
    "                    X_new = pd.DataFrame({\n",
    "                        \"day_of_week\":[tomorrow_date.dayofweek],\n",
    "                        \"is_weekend\":[int(tomorrow_date.dayofweek>=5)],\n",
    "                        \"geo_encoded\":[self.geo_encoder.transform([geo])[0]]\n",
    "                    })\n",
    "                    try:\n",
    "                        yhat = float(self.rf_models[key].predict(X_new)[0])\n",
    "                    except:\n",
    "                        yhat = self.slot_means.get((geo, slot),\n",
    "                                                   self.global_slot_means.get(slot, 0.0))\n",
    "                else:\n",
    "                    # Fallback: pakai mean historis geo-slot > global-slot > 0\n",
    "                    yhat = self.slot_means.get((geo, slot),\n",
    "                                               self.global_slot_means.get(slot, 0.0))\n",
    "                out[\"geo_hash\"].append(geo)\n",
    "                out[\"date\"].append(tomorrow_date)\n",
    "                out[\"slot_30min\"].append(slot)\n",
    "                out[\"predicted_request_count\"].append(max(0,yhat))\n",
    "            results.append(pd.DataFrame(out))\n",
    "        return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# ======================\n",
    "# Merge histori + new geos\n",
    "# ======================\n",
    "    def load_and_preprocess_with_new_geos(self, historical_df, new_df):\n",
    "        df_hist = historical_df.copy() if historical_df is not None else pd.DataFrame()\n",
    "        df_new = new_df.copy() if new_df is not None else pd.DataFrame()\n",
    "\n",
    "        # pastikan kolom ada\n",
    "        for df in [df_hist, df_new]:\n",
    "            for col in [\"request_date\",\"origin_geo_hash\",\"time_slot\",\"request_count\"]:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "\n",
    "        # semua geo unik\n",
    "        all_geos = pd.concat([\n",
    "            df_hist.get(\"origin_geo_hash\", pd.Series(dtype=str)),\n",
    "            df_new.get(\"origin_geo_hash\", pd.Series(dtype=str))\n",
    "        ]).dropna().unique()\n",
    "\n",
    "        last_date_hist = df_hist[\"request_date\"].max() if not df_hist.empty else pd.Timestamp.today()\n",
    "\n",
    "        # tambahkan row geo baru\n",
    "        new_rows = []\n",
    "        for geo in all_geos:\n",
    "            if geo not in df_hist[\"origin_geo_hash\"].unique():\n",
    "                for slot in range(48):\n",
    "                    new_rows.append({\n",
    "                        \"origin_geo_hash\": geo,\n",
    "                        \"request_date\": last_date_hist,\n",
    "                        \"time_slot\": f\"{slot//2:02d}:{(slot%2)*30:02d}-{slot//2:02d}:{(slot%2+1)*30:02d}\",\n",
    "                        \"request_count\": 0\n",
    "                    })\n",
    "        if new_rows:\n",
    "            df_hist = pd.concat([df_hist, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "        df_full = pd.concat([df_hist, df_new], ignore_index=True) if not df_new.empty else df_hist\n",
    "        return self.load_and_preprocess(df_full)\n",
    "# ======================\n",
    "# Fill missing slots untuk tanggal terakhir\n",
    "# ======================\n",
    "    @staticmethod\n",
    "    def fill_missing_slots(df_last_day):\n",
    "        all_slots = np.arange(48)\n",
    "        all_geos = df_last_day[\"origin_geo_hash\"].unique()\n",
    "        new_rows = []\n",
    "        for geo in all_geos:\n",
    "            geo_slots = df_last_day[df_last_day[\"origin_geo_hash\"]==geo][\"slot_30min\"].unique()\n",
    "            missing_slots = set(all_slots) - set(geo_slots)\n",
    "            for slot in missing_slots:\n",
    "                new_rows.append({\n",
    "                    \"origin_geo_hash\": geo,\n",
    "                    \"request_date\": df_last_day[\"request_date\"].max(),\n",
    "                    \"slot_30min\": slot,\n",
    "                    \"request_count\": 0\n",
    "                })\n",
    "        if new_rows:\n",
    "            df_last_day = pd.concat([df_last_day, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "        return df_last_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14f81a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è File data baru tidak ditemukan. Menggunakan histori yang ada saja.\n",
      "Epoch 1/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 217ms/step - loss: 1.2725 - mae: 0.9581 - val_loss: 1.2690 - val_mae: 0.9522 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 136ms/step - loss: 1.0084 - mae: 0.8273 - val_loss: 0.9230 - val_mae: 0.7796 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - loss: 0.7808 - mae: 0.7093 - val_loss: 0.6646 - val_mae: 0.6341 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - loss: 0.6474 - mae: 0.6337 - val_loss: 0.5561 - val_mae: 0.5555 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - loss: 0.5640 - mae: 0.5875 - val_loss: 0.5184 - val_mae: 0.5349 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - loss: 0.5277 - mae: 0.5642 - val_loss: 0.4814 - val_mae: 0.4989 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - loss: 0.4941 - mae: 0.5433 - val_loss: 0.4386 - val_mae: 0.4439 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step - loss: 0.4822 - mae: 0.5309 - val_loss: 0.4441 - val_mae: 0.4571 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - loss: 0.4709 - mae: 0.5247 - val_loss: 0.4686 - val_mae: 0.4985 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step - loss: 0.4586 - mae: 0.5164 - val_loss: 0.4364 - val_mae: 0.4486 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 139ms/step - loss: 0.4463 - mae: 0.5033 - val_loss: 0.4317 - val_mae: 0.4385 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - loss: 0.4486 - mae: 0.5075 - val_loss: 0.4230 - val_mae: 0.4217 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 136ms/step - loss: 0.4475 - mae: 0.5062 - val_loss: 0.4187 - val_mae: 0.4129 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 139ms/step - loss: 0.4411 - mae: 0.5005 - val_loss: 0.4175 - val_mae: 0.4105 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step - loss: 0.4443 - mae: 0.5002 - val_loss: 0.4483 - val_mae: 0.4721 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 137ms/step - loss: 0.4396 - mae: 0.4951 - val_loss: 0.4307 - val_mae: 0.4388 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - loss: 0.4278 - mae: 0.4854 - val_loss: 0.4248 - val_mae: 0.4262 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step - loss: 0.4279 - mae: 0.4870 - val_loss: 0.4233 - val_mae: 0.4209 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 140ms/step - loss: 0.4264 - mae: 0.4863 - val_loss: 0.4172 - val_mae: 0.4100 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - loss: 0.4262 - mae: 0.4811 - val_loss: 0.4178 - val_mae: 0.4138 - learning_rate: 0.0010\n",
      "üîπ RF models trained: 3519\n",
      "‚úÖ Prediksi tersimpan di prediksi_28_09_2025_20251001_141040.csv\n"
     ]
    }
   ],
   "source": [
    "historical_file = \"data_historis.csv\"\n",
    "file_new_data = \"data_29_09_2025.csv\"\n",
    "\n",
    "# Load histori & data baru\n",
    "historical_df = pd.read_csv(historical_file) if Path(historical_file).exists() else pd.DataFrame()\n",
    "new_df = pd.read_csv(file_new_data) if Path(file_new_data).exists() else pd.DataFrame()\n",
    "\n",
    "# Pastikan kolom selalu ada\n",
    "for df in [historical_df, new_df]:\n",
    "    for col in [\"request_date\", \"origin_geo_hash\", \"time_slot\", \"request_count\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    if not df.empty:\n",
    "        df[\"request_date\"] = pd.to_datetime(df[\"request_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "# Gabungkan data baru ke histori\n",
    "if not new_df.empty:\n",
    "    historical_df = pd.concat([historical_df, new_df], ignore_index=True)\n",
    "    historical_df.to_csv(historical_file, index=False)\n",
    "    print(f\"‚úÖ Data baru ditambahkan. Total rows: {len(historical_df)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è File data baru tidak ditemukan. Menggunakan histori yang ada saja.\")\n",
    "\n",
    "# Guard: kalau tidak ada data sama sekali ‚Üí stop\n",
    "if historical_df.empty:\n",
    "    print(\"‚ùå Tidak ada data untuk diproses.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Inisialisasi predictor\n",
    "predictor = TimeSeriesRequestPredictor30MinHybrid()\n",
    "\n",
    "# Preprocess data\n",
    "df_full = predictor.load_and_preprocess_with_new_geos(historical_df, new_df)\n",
    "\n",
    "# Training model\n",
    "train_data, _ = predictor.train(df=df_full, epochs=20)\n",
    "\n",
    "# Ambil tanggal terakhir\n",
    "if df_full[\"request_date\"].notna().any():\n",
    "    max_date = pd.to_datetime(df_full[\"request_date\"]).max()\n",
    "else:\n",
    "    print(\"‚ùå Tidak ada tanggal valid di dataset.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Ambil data terakhir & isi slot kosong\n",
    "df_last_day = df_full[df_full[\"request_date\"] == max_date].copy()\n",
    "df_last_day = predictor.fill_missing_slots(df_last_day)\n",
    "\n",
    "# Prediksi semua geo untuk H+1\n",
    "df_pred = predictor.predict_all_next_day_30min_filtered(df_last_day)\n",
    "df_pred[\"date\"] = pd.to_datetime(df_pred[\"date\"]).dt.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "# Simpan prediksi dengan nama unik (hindari overwrite)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "prediksi_file = f\"prediksi_{max_date.strftime('%d_%m_%Y')}_{timestamp}.csv\"\n",
    "df_pred.to_csv(prediksi_file, index=False)\n",
    "print(f\"‚úÖ Prediksi tersimpan di {prediksi_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9a126b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(pred_file: str, actual_file: str, output_file=\"evaluasi_geo_hash_sama.csv\"):\n",
    "\n",
    "    # --- Baca file prediksi ---\n",
    "    if not Path(pred_file).exists():\n",
    "        print(f\"‚ö†Ô∏è File {pred_file} tidak ditemukan.\")\n",
    "        return\n",
    "    df_pred = pd.read_csv(pred_file)\n",
    "    df_pred[\"date\"] = pd.to_datetime(df_pred[\"date\"], dayfirst=True).dt.date\n",
    "\n",
    "    # --- Baca file actual ---\n",
    "    if not Path(actual_file).exists():\n",
    "        print(f\"‚ö†Ô∏è File {actual_file} tidak ditemukan.\")\n",
    "        return\n",
    "    df_actual = pd.read_csv(actual_file, skipinitialspace=True)\n",
    "    df_actual.columns = df_actual.columns.str.strip()\n",
    "\n",
    "    if \"request_date\" not in df_actual.columns:\n",
    "        print(\"‚ö†Ô∏è Kolom 'request_date' tidak ditemukan di CSV actual!\")\n",
    "        return\n",
    "\n",
    "    # Convert tanggal\n",
    "    df_actual[\"request_date\"] = pd.to_datetime(df_actual[\"request_date\"], dayfirst=True)\n",
    "\n",
    "    # Extract slot\n",
    "    def extract_slot(val):\n",
    "        try:\n",
    "            h, m = map(int, val.split(\"-\")[0].strip().split(\":\"))\n",
    "            return h * 2 + (m // 30)\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    df_actual[\"slot_30min\"] = df_actual[\"time_slot\"].apply(extract_slot)\n",
    "    df_actual[\"geo_hash\"] = df_actual[\"origin_geo_hash\"]\n",
    "    df_actual[\"date\"] = df_actual[\"request_date\"].dt.date\n",
    "    df_actual[\"actual_request_count\"] = df_actual[\"request_count\"]\n",
    "\n",
    "    # Pilih kolom penting\n",
    "    df_actual_proc = df_actual[[\"geo_hash\",\"date\",\"slot_30min\",\"actual_request_count\"]]\n",
    "\n",
    "    # Filter hanya geo_hash yang ada di prediksi\n",
    "    valid_geo = df_pred[\"geo_hash\"].unique()\n",
    "    df_actual_proc = df_actual_proc[df_actual_proc[\"geo_hash\"].isin(valid_geo)]\n",
    "\n",
    "    # Merge inner\n",
    "    merged = pd.merge(\n",
    "        df_pred,\n",
    "        df_actual_proc,\n",
    "        on=[\"geo_hash\",\"date\",\"slot_30min\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    if not merged.empty:\n",
    "        mae = mean_absolute_error(merged[\"actual_request_count\"], merged[\"predicted_request_count\"])\n",
    "        r2  = r2_score(merged[\"actual_request_count\"], merged[\"predicted_request_count\"])\n",
    "        rmse = sqrt(mean_squared_error(merged[\"actual_request_count\"], merged[\"predicted_request_count\"]))\n",
    "        print(f\"üìä Evaluasi (geo_hash sama): MAE={mae:.4f}, R¬≤={r2:.4f}, RMSE={rmse:.4f}, N={len(merged)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Tidak ada slot/geo_hash yang cocok untuk evaluasi.\")\n",
    "\n",
    "    # Simpan hasil evaluasi\n",
    "    merged.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Hasil evaluasi disimpan di {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1af83e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluasi (geo_hash sama): MAE=2.7525, R¬≤=-0.3137, RMSE=4.0944, N=1083\n",
      "‚úÖ Hasil evaluasi disimpan di evaluasi_manual.csv\n"
     ]
    }
   ],
   "source": [
    "evaluate_prediction(\n",
    "    pred_file=\"prediksi_28_09_2025_20251001_141040.csv\",\n",
    "    actual_file=\"data_29_09_202.csv\",\n",
    "    output_file=\"evaluasi_manual.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from datetime import timedelta\n",
    "\n",
    "class RandomForestPredictor:\n",
    "    def __init__(self, history_file, today_file, use_today=False):\n",
    "        self.history_file = history_file\n",
    "        self.today_file = today_file\n",
    "        self.use_today = use_today\n",
    "        self.model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=1, verbose=1)\n",
    "\n",
    "        # Encoders\n",
    "        self.geohash_encoder = LabelEncoder()\n",
    "\n",
    "    def _load_data(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Bersihkan nama kolom dari spasi & tanda kutip\n",
    "        df.columns = df.columns.str.strip().str.replace('\"', '').str.replace(\"'\", \"\")\n",
    "\n",
    "        # Rename kolom sesuai format internal\n",
    "        df.rename(columns={\n",
    "            \"request_date\": \"date\",\n",
    "            \"origin_geo_hash\": \"geohash\",\n",
    "            \"time_slot\": \"time_slot\",\n",
    "            \"request_count\": \"count\"\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Pastikan kolom wajib ada\n",
    "        required = {\"date\", \"time_slot\", \"geohash\", \"count\"}\n",
    "        missing = required - set(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"CSV {file_path} missing required columns: {missing}\")\n",
    "\n",
    "        # Parse date\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors='coerce', dayfirst=True)\n",
    "\n",
    "        # Hapus data dengan count 0\n",
    "        if \"count\" in df.columns:\n",
    "            df = df[df[\"count\"] > 0]\n",
    "\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def _create_full_grid(self, df):\n",
    "        # Comment to remove filling 0\n",
    "        dates = df[\"date\"].unique()\n",
    "        timeslots = df[\"time_slot\"].unique()\n",
    "        geohashes = df[\"geohash\"].unique()\n",
    "        \n",
    "        grid = pd.MultiIndex.from_product([dates, timeslots, geohashes], names=[\"date\", \"time_slot\", \"geohash\"]).to_frame(index=False)\n",
    "\n",
    "        df = pd.merge(grid, df, on=[\"date\", \"time_slot\", \"geohash\"], how=\"left\")\n",
    "        df[\"count\"] = df[\"count\"].fillna(0)\n",
    "\n",
    "        df.to_csv(self.history_file, index=False)\n",
    "        return df\n",
    "\n",
    "    def preprocess(self):\n",
    "        # Load data\n",
    "        history = self._load_data(self.history_file)\n",
    "        today = self._load_data(self.today_file)\n",
    "\n",
    "        # Combine history + today\n",
    "        combined = pd.concat([history, today], ignore_index=True)\n",
    "\n",
    "        # Fill missing combinations for the whole dataset\n",
    "        combined = self._create_full_grid(combined)\n",
    "\n",
    "        # Parse timeslot\n",
    "        combined[\"slot_start\"] = combined[\"time_slot\"].str.split(\"-\").str[0]\n",
    "        combined[\"slot_start\"] = pd.to_datetime(combined[\"slot_start\"], format=\"%H:%M\")\n",
    "        combined[\"hour\"] = combined[\"slot_start\"].dt.hour\n",
    "        combined[\"minute\"] = combined[\"slot_start\"].dt.minute\n",
    "        combined.drop(columns=[\"slot_start\"], inplace=True)\n",
    "\n",
    "        # Encode geohash\n",
    "        combined[\"geohash_encoded\"] = self.geohash_encoder.fit_transform(combined[\"geohash\"])\n",
    "\n",
    "        # Extract date features\n",
    "        combined[\"day\"] = combined[\"date\"].dt.day\n",
    "        combined[\"month\"] = combined[\"date\"].dt.month\n",
    "        combined[\"weekday\"] = combined[\"date\"].dt.weekday\n",
    "\n",
    "        # Split back into history and today\n",
    "        history = combined[combined[\"date\"] < combined[\"date\"].max()]\n",
    "        today = combined[combined[\"date\"] == combined[\"date\"].max()]\n",
    "\n",
    "        return combined, history[\"date\"].max(), today[\"date\"].max()\n",
    "    \n",
    "    def train_and_predict(self, target_date=None):\n",
    "        combined, last_history_date, today_date = self.preprocess()\n",
    "\n",
    "        # Tentukan tanggal target prediksi\n",
    "        if target_date is None:\n",
    "            # default: besok dari hari terakhir\n",
    "            target_date = (today_date if self.use_today else last_history_date) + timedelta(days=1)\n",
    "        else:\n",
    "            target_date = pd.to_datetime(target_date)\n",
    "\n",
    "        # Split kembali history vs today\n",
    "        history = combined[combined[\"date\"] <= last_history_date].copy()\n",
    "        today = combined[combined[\"date\"] == today_date].copy()\n",
    "\n",
    "        if self.use_today:\n",
    "            train = pd.concat([history, today], ignore_index=True)\n",
    "        else:\n",
    "            train = history\n",
    "\n",
    "        # Features untuk training\n",
    "        features = [\"geohash_encoded\", \"hour\", \"minute\", \"day\", \"month\", \"weekday\"]\n",
    "        X_train = train[features]\n",
    "        y_train = train[\"count\"]\n",
    "\n",
    "        # Train model\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "        # Siapkan data prediksi untuk target_date\n",
    "        if target_date == today_date:\n",
    "            pred_df = today.copy()\n",
    "        else:\n",
    "            # Ambil data terakhir, ubah tanggal ke target_date\n",
    "            pred_df = combined[combined[\"date\"] == today_date].copy()\n",
    "            pred_df[\"date\"] = target_date\n",
    "\n",
    "        X_pred = pred_df[features]\n",
    "        pred_df[\"prediction\"] = self.model.predict(X_pred)\n",
    "\n",
    "        # Save hasil prediksi\n",
    "        pred_file = f\"forest_{target_date.strftime('%Y-%m-%d')}_prediction.csv\"\n",
    "        pred_df[[\"date\", \"time_slot\", \"geohash\", \"prediction\"]].to_csv(pred_file, index=False)\n",
    "\n",
    "        print(f\"‚úÖ Prediction saved to {pred_file}\")\n",
    "        return pred_file\n",
    "\n",
    "\n",
    "    def evaluate_by_file(pred_file, actual_file, output_file=\"eval_manual.csv\"):\n",
    "        import pandas as pd\n",
    "        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "        import numpy as np\n",
    "\n",
    "        # --- Load prediksi ---\n",
    "        pred_df = pd.read_csv(pred_file)\n",
    "        pred_df.rename(columns={\"prediction\": \"count_pred\"}, inplace=True)\n",
    "        pred_df[\"date\"] = pd.to_datetime(pred_df[\"date\"], format=\"%Y-%m-%d\").dt.date\n",
    "\n",
    "        # --- Load actual ---\n",
    "        actual_df = pd.read_csv(actual_file)\n",
    "        actual_df.rename(columns={\n",
    "            \"request_date\": \"date\",\n",
    "            \"origin_geo_hash\": \"geohash\",\n",
    "            \"request_count\": \"count_actual\"\n",
    "        }, inplace=True)\n",
    "        actual_df[\"date\"] = pd.to_datetime(actual_df[\"date\"], dayfirst=True).dt.date\n",
    "\n",
    "        # Merge\n",
    "        eval_df = pred_df.merge(actual_df, on=[\"date\", \"time_slot\", \"geohash\"], how=\"inner\")\n",
    "\n",
    "        if eval_df.empty:\n",
    "            print(\"‚ö† Tidak ada data cocok untuk evaluasi (cek date/time_slot/geohash).\")\n",
    "            return\n",
    "\n",
    "        mae = mean_absolute_error(eval_df[\"count_actual\"], eval_df[\"count_pred\"])\n",
    "        rmse = np.sqrt(mean_squared_error(eval_df[\"count_actual\"], eval_df[\"count_pred\"]))\n",
    "        r2 = r2_score(eval_df[\"count_actual\"], eval_df[\"count_pred\"])\n",
    "\n",
    "        print(f\"üìä Evaluasi: MAE={mae:.2f}, RMSE={rmse:.2f}, R¬≤={r2:.2f}, N={len(eval_df)}\")\n",
    "\n",
    "        eval_df.to_csv(output_file, index=False)\n",
    "        print(f\"‚úÖ Hasil evaluasi disimpan di {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inisialisasi predictor ---\n",
    "predictor = RandomForestPredictor(\n",
    "    history_file=\"data_historis.csv\",\n",
    "    today_file=\"data_29_09_202.csv\",\n",
    "    use_today=False  # supaya training cuma sampai 28\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4db6ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:   37.4s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prediction saved to forest_2025-09-29_prediction.csv\n",
      "‚úÖ Hasil evaluasi disimpan di eval_2025-09-29.csv\n",
      "üìä MAE=3.0862, RMSE=4.5956, R¬≤=-0.7168, N=1163\n"
     ]
    }
   ],
   "source": [
    "# Prediksi 29-09-2025\n",
    "pred_file = predictor.train_and_predict(target_date=\"2025-09-29\")\n",
    "\n",
    "# Evaluasi lawan data aktual\n",
    "evaluate_by_file(\n",
    "    pred_file=pred_file,\n",
    "    actual_file=\"data_29_09_202.csv\",\n",
    "    output_file=\"eval_2025-09-29.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
