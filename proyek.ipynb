{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e4c7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2afdb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimeSeriesRequestPredictor30Min:\n",
    "    def __init__(self, sequence_length=144, prediction_horizon=48):\n",
    "        \"\"\"\n",
    "        sequence_length: panjang window historis (slot 30 menit)\n",
    "        prediction_horizon: horizon prediksi (berapa slot 30 menit ke depan)\n",
    "        Default 144 slot = 3 hari * 48 slot/hari\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.scaler = StandardScaler()\n",
    "        self.geo_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    # ================================\n",
    "    # Preprocessing\n",
    "    # ================================\n",
    "    def load_and_preprocess_data(self, csv_file_path):\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        df[\"request_date\"] = pd.to_datetime(df[\"request_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "        # Ekstrak slot 30 menit\n",
    "        def extract_slot(val):\n",
    "            if pd.isna(val):\n",
    "                return -1\n",
    "            if isinstance(val, str) and \"-\" in val:\n",
    "                start = val.split(\"-\")[0]\n",
    "                h, m = map(int, start.split(\":\"))\n",
    "                return h*2 + (m//30)\n",
    "            try:\n",
    "                t = pd.to_datetime(val)\n",
    "                return t.hour*2 + (t.minute//30)\n",
    "            except:\n",
    "                return -1\n",
    "        df[\"slot_30min\"] = df[\"time_slot\"].apply(extract_slot)\n",
    "        df[\"is_missing_slot\"] = (df[\"slot_30min\"] == -1).astype(int)\n",
    "\n",
    "        # Encode geo_hash\n",
    "        df[\"geo_encoded\"] = self.geo_encoder.fit_transform(df[\"origin_geo_hash\"])\n",
    "\n",
    "        # Fitur temporal\n",
    "        df[\"day_of_week\"] = df[\"request_date\"].dt.dayofweek\n",
    "        df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype(int)\n",
    "\n",
    "        df = df.sort_values([\"origin_geo_hash\", \"request_date\", \"slot_30min\"])\n",
    "        return df\n",
    "\n",
    "    # ================================\n",
    "    # Sequence Generator dengan log-transform\n",
    "    # ================================\n",
    "    def create_sequences(self, data):\n",
    "        sequences, targets = [], []\n",
    "        features = [\"request_count\",\"slot_30min\",\"day_of_week\",\"is_weekend\",\"geo_encoded\"]\n",
    "\n",
    "        for geo_hash in data[\"origin_geo_hash\"].unique():\n",
    "            geo_data = data[data[\"origin_geo_hash\"]==geo_hash].sort_values([\"request_date\",\"slot_30min\"])\n",
    "            feature_data = geo_data[features].values.copy()\n",
    "\n",
    "            # Log-transform request_count\n",
    "            feature_data[:,0] = np.log1p(feature_data[:,0])\n",
    "\n",
    "            for i in range(len(feature_data) - self.sequence_length - self.prediction_horizon + 1):\n",
    "                seq = feature_data[i:i+self.sequence_length]\n",
    "                target = feature_data[i+self.sequence_length:i+self.sequence_length+self.prediction_horizon,0]\n",
    "                sequences.append(seq)\n",
    "                targets.append(target)\n",
    "\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    # ================================\n",
    "    # Build model\n",
    "    # ================================\n",
    "    def build_model(self, input_shape):\n",
    "        model = Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            LSTM(128, return_sequences=True, recurrent_dropout=0.2),\n",
    "            LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(self.prediction_horizon, activation=\"linear\")\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "        return model\n",
    "\n",
    "    # ================================\n",
    "    # Temporal split\n",
    "    # ================================\n",
    "    def temporal_split(self, X, y, val_ratio=0.2):\n",
    "        split_index = int(len(X)*(1-val_ratio))\n",
    "        return X[:split_index], X[split_index:], y[:split_index], y[split_index:]\n",
    "\n",
    "    # ================================\n",
    "    # Training\n",
    "    # ================================\n",
    "    def train(self, csv_file_path, validation_split=0.2, epochs=50):\n",
    "        data = self.load_and_preprocess_data(csv_file_path)\n",
    "        X, y = self.create_sequences(data)\n",
    "\n",
    "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "        X_scaled = self.scaler.fit_transform(X_reshaped).reshape(X.shape)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = self.temporal_split(X_scaled, y, validation_split)\n",
    "        self.model = self.build_model((X.shape[1], X.shape[2]))\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "        ]\n",
    "\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return data, history\n",
    "\n",
    "    # ================================\n",
    "    # Prediksi per geo_hash\n",
    "    # ================================\n",
    "    def predict_next_day_30min_filtered(self, data, geo_hash):\n",
    "        \"\"\"\n",
    "        Prediksi hanya untuk slot yang pernah ada di data historis.\n",
    "        Jika data < sequence_length → tetap ditampilkan di CSV, \n",
    "        tapi predicted_request_count = NaN.\n",
    "        \"\"\"\n",
    "        geo_data = data[data[\"origin_geo_hash\"]==geo_hash].sort_values([\"request_date\",\"slot_30min\"])\n",
    "        features = [\"request_count\",\"slot_30min\",\"day_of_week\",\"is_weekend\",\"geo_encoded\"]\n",
    "\n",
    "        # Slot historis yang ada\n",
    "        existing_slots = geo_data[\"slot_30min\"].unique()\n",
    "        tomorrow_date = geo_data[\"request_date\"].max() + timedelta(days=1) if len(geo_data) > 0 else pd.NaT\n",
    "\n",
    "        # Kalau data terlalu sedikit → return dataframe dengan NaN\n",
    "        if len(geo_data) < self.sequence_length or self.model is None:\n",
    "            return pd.DataFrame({\n",
    "                \"geo_hash\": [geo_hash]*len(existing_slots),\n",
    "                \"date\": [tomorrow_date]*len(existing_slots),\n",
    "                \"slot_30min\": existing_slots,\n",
    "                \"predicted_request_count\": [0]*len(existing_slots)   # bisa diganti 0 kalau mau\n",
    "            })\n",
    "\n",
    "        # Ambil sequence terakhir untuk prediksi\n",
    "        last_seq = geo_data[features].tail(self.sequence_length).values\n",
    "        last_seq[:,0] = np.log1p(last_seq[:,0])  # log-transform\n",
    "        seq_scaled = self.scaler.transform(last_seq).reshape(1,self.sequence_length,len(features))\n",
    "        pred_full = self.model.predict(seq_scaled, verbose=0)[0]\n",
    "\n",
    "        # Inverse log-transform\n",
    "        pred_full = np.expm1(pred_full)\n",
    "        pred_full = np.maximum(0, pred_full)\n",
    "\n",
    "        # Buat dataframe hasil prediksi\n",
    "        pred_dict = {\n",
    "            \"geo_hash\": [],\n",
    "            \"date\": [],\n",
    "            \"slot_30min\": [],\n",
    "            \"predicted_request_count\": []\n",
    "        }\n",
    "\n",
    "        for i, slot in enumerate(range(len(pred_full))):\n",
    "            if slot in existing_slots:\n",
    "                pred_dict[\"geo_hash\"].append(geo_hash)\n",
    "                pred_dict[\"date\"].append(tomorrow_date)\n",
    "                pred_dict[\"slot_30min\"].append(slot)\n",
    "                pred_dict[\"predicted_request_count\"].append(pred_full[i])\n",
    "\n",
    "        # bikin dataframe & sort\n",
    "        df_result = pd.DataFrame(pred_dict)\n",
    "        df_result = df_result.sort_values([\"geo_hash\", \"date\", \"slot_30min\"]).reset_index(drop=True)\n",
    "        return df_result\n",
    "        \n",
    "    # ================================\n",
    "    # Prediksi semua geo_hash\n",
    "    # ================================\n",
    "    def predict_all_next_day_30min_filtered(self, data, fallback_value=np.nan):\n",
    "        \"\"\"\n",
    "        Prediksi untuk semua geo_hash, hanya untuk slot yang ada di historis.\n",
    "        Jika histori < sequence_length → tetap ditampilkan dengan predicted_request_count = NaN/0.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for geo_hash in data[\"origin_geo_hash\"].unique():\n",
    "            df_pred = self.predict_next_day_30min_filtered(data, geo_hash)\n",
    "\n",
    "            # Kalau hasil kosong (misalnya data kosong) → buat dummy NaN/0\n",
    "            if df_pred.empty:\n",
    "                geo_data = data[data[\"origin_geo_hash\"]==geo_hash]\n",
    "                existing_slots = geo_data[\"slot_30min\"].unique()\n",
    "                tomorrow_date = geo_data[\"request_date\"].max() + timedelta(days=1) if len(geo_data) > 0 else pd.NaT\n",
    "                df_pred = pd.DataFrame({\n",
    "                    \"geo_hash\": [geo_hash]*len(existing_slots),\n",
    "                    \"date\": [tomorrow_date]*len(existing_slots),\n",
    "                    \"slot_30min\": existing_slots,\n",
    "                    \"predicted_request_count\": [fallback_value]*len(existing_slots)\n",
    "                })\n",
    "\n",
    "            results.append(df_pred)\n",
    "\n",
    "        if results:\n",
    "                df_all = pd.concat(results, ignore_index=True)\n",
    "                df_all = df_all.sort_values([\"geo_hash\", \"date\", \"slot_30min\"]).reset_index(drop=True)\n",
    "                return df_all\n",
    "        else:\n",
    "                return pd.DataFrame(columns=[\"geo_hash\", \"date\", \"slot_30min\", \"predicted_request_count\"])\n",
    "        \n",
    "    # ================================\n",
    "    # Evaluasi\n",
    "    # ================================\n",
    "    def evaluate_yesterday_30min(self, data, yesterday_predictions):\n",
    "        actuals = data.merge(\n",
    "            yesterday_predictions[[\"geo_hash\",\"date\",\"slot_30min\"]],\n",
    "            left_on=[\"origin_geo_hash\",\"request_date\",\"slot_30min\"],\n",
    "            right_on=[\"geo_hash\",\"date\",\"slot_30min\"],\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        actuals = actuals.rename(columns={\"request_count\":\"actual_request_count\"})\n",
    "        merged = pd.merge(yesterday_predictions, actuals, on=[\"geo_hash\",\"date\",\"slot_30min\"], how=\"inner\")\n",
    "\n",
    "        if merged.empty:\n",
    "            print(\"⚠️ Tidak ada data aktual untuk dievaluasi.\")\n",
    "            return None\n",
    "\n",
    "        mae = mean_absolute_error(merged[\"actual_request_count\"], merged[\"predicted_request_count\"])\n",
    "        rmse = mean_squared_error(merged[\"actual_request_count\"], merged[\"predicted_request_count\"], squared=False)\n",
    "        print(f\"Evaluasi untuk {merged['date'].iloc[0].date()}: MAE={mae:.2f}, RMSE={rmse:.2f}\")\n",
    "        return merged, {\"MAE\": mae, \"RMSE\": rmse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f81a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ File data_29_09_2025.csv tidak ditemukan, menggunakan histori yang ada saja.\n",
      "Epoch 1/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 189ms/step - loss: 1.2939 - mae: 0.9672 - val_loss: 1.4443 - val_mae: 1.0402 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 1.0542 - mae: 0.8539 - val_loss: 0.9475 - val_mae: 0.7953 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.8254 - mae: 0.7280 - val_loss: 0.6999 - val_mae: 0.6651 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.6573 - mae: 0.6442 - val_loss: 0.5674 - val_mae: 0.5800 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 0.5688 - mae: 0.5920 - val_loss: 0.4974 - val_mae: 0.5225 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 0.5250 - mae: 0.5633 - val_loss: 0.4779 - val_mae: 0.5108 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.4960 - mae: 0.5425 - val_loss: 0.4554 - val_mae: 0.4820 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 0.4822 - mae: 0.5335 - val_loss: 0.4449 - val_mae: 0.4661 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.4725 - mae: 0.5243 - val_loss: 0.4388 - val_mae: 0.4553 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.4627 - mae: 0.5153 - val_loss: 0.4425 - val_mae: 0.4634 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 0.4585 - mae: 0.5125 - val_loss: 0.4448 - val_mae: 0.4685 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 0.4513 - mae: 0.5052 - val_loss: 0.4478 - val_mae: 0.4735 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.4447 - mae: 0.5000 - val_loss: 0.4295 - val_mae: 0.4369 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.4401 - mae: 0.4973 - val_loss: 0.4216 - val_mae: 0.4202 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.4427 - mae: 0.4981 - val_loss: 0.4293 - val_mae: 0.4394 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 0.4321 - mae: 0.4893 - val_loss: 0.4239 - val_mae: 0.4258 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 0.4363 - mae: 0.4925 - val_loss: 0.4198 - val_mae: 0.4151 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.4318 - mae: 0.4852 - val_loss: 0.4201 - val_mae: 0.4163 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.4316 - mae: 0.4844 - val_loss: 0.4230 - val_mae: 0.4234 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.4317 - mae: 0.4880 - val_loss: 0.4356 - val_mae: 0.4523 - learning_rate: 0.0010\n",
      "✅ Model selesai di-train.\n",
      "✅ Prediksi untuk 29_09_2025 tersimpan di prediksi_29_09_2025.csv\n",
      "⚠️ Data aktual untuk 28_09_2025 belum tersedia atau data kosong, evaluasi dilewati.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 1️⃣ Load dan update histori\n",
    "# =============================\n",
    "historical_file = \"data_historis.csv\"\n",
    "file_new_data = \"data_29_09_2025.csv\"  # ganti sesuai data hari ini\n",
    "\n",
    "# Load data historis\n",
    "if Path(historical_file).exists():\n",
    "    historical_data = pd.read_csv(historical_file)\n",
    "else:\n",
    "    historical_data = pd.DataFrame()  # kalau histori belum ada, buat dataframe kosong\n",
    "    print(\"⚠️ File histori belum ada, membuat dataframe kosong.\")\n",
    "\n",
    "# Tambahkan data baru jika ada\n",
    "if Path(file_new_data).exists():\n",
    "    new_data = pd.read_csv(file_new_data)\n",
    "    historical_data = pd.concat([historical_data, new_data], ignore_index=True)\n",
    "    historical_data.to_csv(historical_file, index=False)\n",
    "    print(f\"✅ Data baru dari {file_new_data} ditambahkan ke histori. Total rows: {len(historical_data)}\")\n",
    "else:\n",
    "    print(f\"⚠️ File {file_new_data} tidak ditemukan, menggunakan histori yang ada saja.\")\n",
    "\n",
    "# =============================\n",
    "# 2️⃣ Inisialisasi dan train model\n",
    "# =============================\n",
    "predictor = TimeSeriesRequestPredictor30Min(sequence_length=144, prediction_horizon=48)\n",
    "\n",
    "if len(historical_data) > 0:\n",
    "    data_preprocessed, history = predictor.train(historical_file, epochs=20)\n",
    "    print(\"✅ Model selesai di-train.\")\n",
    "else:\n",
    "    data_preprocessed = pd.DataFrame()  # kosong jika tidak ada data\n",
    "\n",
    "# =============================\n",
    "# 3️⃣ Prediksi hari berikutnya\n",
    "# =============================\n",
    "if len(data_preprocessed) > 0:\n",
    "    tomorrow_date = (data_preprocessed[\"request_date\"].max() + timedelta(days=1)).strftime(\"%d_%m_%Y\")\n",
    "    predictions = predictor.predict_all_next_day_30min_filtered(data_preprocessed)\n",
    "\n",
    "    # Simpan hasil prediksi\n",
    "    pred_file = f\"prediksi_{tomorrow_date}.csv\"\n",
    "    predictions.to_csv(pred_file, index=False)\n",
    "    print(f\"✅ Prediksi untuk {tomorrow_date} tersimpan di {pred_file}\")\n",
    "else:\n",
    "    print(\"⚠️ Tidak ada data untuk prediksi.\")\n",
    "\n",
    "# =============================\n",
    "# 4️⃣ Evaluasi prediksi kemarin\n",
    "# =============================\n",
    "yesterday_date = (data_preprocessed[\"request_date\"].max()).strftime(\"%d_%m_%Y\") if len(data_preprocessed) > 0 else None\n",
    "file_yesterday_actual = f\"data_{yesterday_date}.csv\" if yesterday_date else None\n",
    "\n",
    "if file_yesterday_actual and Path(file_yesterday_actual).exists() and len(data_preprocessed) > 0:\n",
    "    actual_yesterday = predictor.load_and_preprocess_data(file_yesterday_actual)\n",
    "    eval_results, metrics = predictor.evaluate_yesterday_30min(actual_yesterday, predictions)\n",
    "else:\n",
    "    print(f\"⚠️ Data aktual untuk {yesterday_date} belum tersedia atau data kosong, evaluasi dilewati.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
