{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e4c7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from pathlib import Path\n",
    "from numpy import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Predictor Class\n",
    "# ==============================\n",
    "class TimeSeriesRequestPredictor30MinHybrid:\n",
    "    def __init__(self, sequence_length=48, prediction_horizon=48, rare_threshold=10):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.rare_threshold = rare_threshold\n",
    "        self.scaler = StandardScaler()\n",
    "        self.geo_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.rf_models = {}\n",
    "        self.slot_counts = {}\n",
    "\n",
    "    # Preprocess\n",
    "    def load_and_preprocess(self, df):\n",
    "        if df.empty:\n",
    "            # buat kolom tetap ada\n",
    "            for col in [\"request_date\",\"origin_geo_hash\",\"time_slot\",\"request_count\"]:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "            return df\n",
    "\n",
    "        # convert date\n",
    "        df[\"request_date\"] = pd.to_datetime(df[\"request_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "        # slot_30min\n",
    "        def extract_slot(val):\n",
    "            if pd.isna(val):\n",
    "                return -1\n",
    "            if isinstance(val, str) and \"-\" in val:\n",
    "                try:\n",
    "                    h, m = map(int, val.split(\"-\")[0].strip().split(\":\"))\n",
    "                    return h*2 + m//30\n",
    "                except:\n",
    "                    return -1\n",
    "            return -1\n",
    "        df[\"slot_30min\"] = df[\"time_slot\"].apply(extract_slot)\n",
    "        df[\"is_missing_slot\"] = (df[\"slot_30min\"]==-1).astype(int)\n",
    "\n",
    "        # Encode geo\n",
    "        if df[\"origin_geo_hash\"].notna().sum() > 0:\n",
    "            df[\"geo_encoded\"] = self.geo_encoder.fit_transform(df[\"origin_geo_hash\"])\n",
    "        else:\n",
    "            df[\"geo_encoded\"] = 0\n",
    "\n",
    "        df[\"day_of_week\"] = df[\"request_date\"].dt.dayofweek.fillna(0).astype(int)\n",
    "        df[\"is_weekend\"] = (df[\"day_of_week\"] >=5).astype(int)\n",
    "\n",
    "        df = df.sort_values([\"origin_geo_hash\",\"request_date\",\"slot_30min\"])\n",
    "        return df\n",
    "\n",
    "    # Merge histori + new, tambah geo baru jika perlu\n",
    "    def load_and_preprocess_with_new_geos(self, historical_df, new_df):\n",
    "        df_hist = historical_df.copy() if historical_df is not None else pd.DataFrame()\n",
    "        df_new = new_df.copy() if new_df is not None else pd.DataFrame()\n",
    "\n",
    "        # pastikan kolom\n",
    "        for df in [df_hist, df_new]:\n",
    "            for col in [\"request_date\",\"origin_geo_hash\",\"time_slot\",\"request_count\"]:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "\n",
    "        # semua geo unik\n",
    "        all_geos = pd.concat([df_hist.get(\"origin_geo_hash\", pd.Series(dtype=str)),\n",
    "                      df_new.get(\"origin_geo_hash\", pd.Series(dtype=str))]).dropna().unique()\n",
    "        last_date_hist = df_hist[\"request_date\"].max() if not df_hist.empty else pd.Timestamp.today()\n",
    "\n",
    "        # tambahkan row geo baru\n",
    "        new_rows = []\n",
    "        for geo in all_geos:\n",
    "            if geo not in df_hist[\"origin_geo_hash\"].unique():\n",
    "                for slot in range(48):\n",
    "                    new_rows.append({\n",
    "                        \"origin_geo_hash\": geo,\n",
    "                        \"request_date\": last_date_hist,\n",
    "                        \"time_slot\": f\"{slot//2:02d}:{(slot%2)*30:02d}-{slot//2:02d}:{(slot%2+1)*30:02d}\",\n",
    "                        \"request_count\": 0\n",
    "                    })\n",
    "        if new_rows:\n",
    "            df_hist = pd.concat([df_hist, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "        df_full = pd.concat([df_hist, df_new], ignore_index=True) if not df_new.empty else df_hist\n",
    "        return self.load_and_preprocess(df_full)\n",
    "\n",
    "    # Fill missing slots untuk tanggal terakhir\n",
    "    @staticmethod\n",
    "    def fill_missing_slots(df_last_day):\n",
    "        all_slots = np.arange(48)\n",
    "        all_geos = df_last_day[\"origin_geo_hash\"].unique()\n",
    "        new_rows = []\n",
    "        for geo in all_geos:\n",
    "            geo_slots = df_last_day[df_last_day[\"origin_geo_hash\"]==geo][\"slot_30min\"].unique()\n",
    "            missing_slots = set(all_slots) - set(geo_slots)\n",
    "            for slot in missing_slots:\n",
    "                new_rows.append({\n",
    "                    \"origin_geo_hash\": geo,\n",
    "                    \"request_date\": df_last_day[\"request_date\"].max(),\n",
    "                    \"slot_30min\": slot,\n",
    "                    \"request_count\": 0\n",
    "                })\n",
    "        if new_rows:\n",
    "            df_last_day = pd.concat([df_last_day, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "        return df_last_day\n",
    "\n",
    "    # Train\n",
    "    def train(self, df=None, epochs=20, validation_split=0.2):\n",
    "        if df is None or df.empty:\n",
    "            print(\"‚ö†Ô∏è Tidak ada data untuk train.\")\n",
    "            return df, None\n",
    "\n",
    "        # hitung slot counts\n",
    "        counts = df.groupby([\"origin_geo_hash\",\"slot_30min\"]).size()\n",
    "        self.slot_counts = counts.to_dict()\n",
    "\n",
    "        # sequence generator\n",
    "        X, y = self.create_sequences(df)\n",
    "        if X.size != 0:\n",
    "            X_scaled = self.scaler.fit_transform(X.reshape(-1,X.shape[-1])).reshape(X.shape)\n",
    "            split_index = int(len(X_scaled)*(1-validation_split))\n",
    "            X_train, X_val = X_scaled[:split_index], X_scaled[split_index:]\n",
    "            y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "            # build LSTM\n",
    "            self.model = self.build_model((X.shape[1], X.shape[2]))\n",
    "            callbacks = [\n",
    "                EarlyStopping(patience=10, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "            ]\n",
    "            self.model.fit(X_train, y_train, validation_data=(X_val,y_val),\n",
    "                           epochs=epochs, batch_size=32, callbacks=callbacks, verbose=1)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Tidak cukup data untuk LSTM.\")\n",
    "\n",
    "        # RF untuk rare slots\n",
    "        for (geo, slot), cnt in self.slot_counts.items():\n",
    "            if cnt < self.rare_threshold:\n",
    "                slot_df = df[(df[\"origin_geo_hash\"]==geo) & (df[\"slot_30min\"]==slot)]\n",
    "                if len(slot_df) >= 3:\n",
    "                    X_rf = slot_df[[\"day_of_week\",\"is_weekend\",\"geo_encoded\"]]\n",
    "                    y_rf = slot_df[\"request_count\"]\n",
    "                    rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "                    rf.fit(X_rf, y_rf)\n",
    "                    self.rf_models[(geo, slot)] = rf\n",
    "\n",
    "        print(f\"üîπ RF models trained: {len(self.rf_models)}\")\n",
    "        return df, None\n",
    "\n",
    "    # Buat sequences\n",
    "    def create_sequences(self, data):\n",
    "        if data.empty:\n",
    "            return np.array([]), np.array([])\n",
    "        sequences, targets = [], []\n",
    "        features = [\"request_count\",\"slot_30min\",\"day_of_week\",\"is_weekend\",\"geo_encoded\"]\n",
    "        for geo in data[\"origin_geo_hash\"].unique():\n",
    "            geo_data = data[data[\"origin_geo_hash\"]==geo].sort_values([\"request_date\",\"slot_30min\"])\n",
    "            feature_data = geo_data[features].fillna(0).values.copy()\n",
    "            feature_data[:,0] = np.log1p(feature_data[:,0] + 1e-6)\n",
    "            for i in range(len(feature_data)-self.sequence_length-self.prediction_horizon+1):\n",
    "                seq = feature_data[i:i+self.sequence_length]\n",
    "                target = feature_data[i+self.sequence_length:i+self.sequence_length+self.prediction_horizon,0]\n",
    "                sequences.append(seq)\n",
    "                targets.append(target)\n",
    "        if len(sequences)==0:\n",
    "            return np.array([]), np.array([])\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    # Build LSTM\n",
    "    def build_model(self, input_shape):\n",
    "        model = Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            LSTM(128, return_sequences=True, recurrent_dropout=0.2),\n",
    "            LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(self.prediction_horizon, activation=\"linear\")\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "        return model\n",
    "\n",
    "    # Prediksi semua geo untuk H+1\n",
    "    def predict_all_next_day_30min_filtered(self, df_last_day, fallback_value=0.0):\n",
    "        if df_last_day.empty:\n",
    "            return pd.DataFrame(columns=[\"geo_hash\",\"date\",\"slot_30min\",\"predicted_request_count\"])\n",
    "\n",
    "        results = []\n",
    "        for geo in df_last_day[\"origin_geo_hash\"].unique():\n",
    "            geo_data = df_last_day[df_last_day[\"origin_geo_hash\"]==geo].sort_values(\"slot_30min\")\n",
    "            last_date = geo_data[\"request_date\"].max()\n",
    "            tomorrow_date = last_date + timedelta(days=1)\n",
    "            all_slots = np.arange(48)\n",
    "\n",
    "            out = {\"geo_hash\":[],\"date\":[],\"slot_30min\":[],\"predicted_request_count\":[]}\n",
    "            for slot in all_slots:\n",
    "                key = (geo,int(slot))\n",
    "                if key in self.rf_models:\n",
    "                    X_new = pd.DataFrame({\n",
    "                        \"day_of_week\":[tomorrow_date.dayofweek],\n",
    "                        \"is_weekend\":[int(tomorrow_date.dayofweek>=5)],\n",
    "                        \"geo_encoded\":[self.geo_encoder.transform([geo])[0]]\n",
    "                    })\n",
    "                    try:\n",
    "                        yhat = float(self.rf_models[key].predict(X_new)[0])\n",
    "                    except:\n",
    "                        yhat = fallback_value\n",
    "                else:\n",
    "                    yhat = fallback_value\n",
    "                out[\"geo_hash\"].append(geo)\n",
    "                out[\"date\"].append(tomorrow_date)\n",
    "                out[\"slot_30min\"].append(slot)\n",
    "                out[\"predicted_request_count\"].append(max(0,yhat))\n",
    "            results.append(pd.DataFrame(out))\n",
    "        return pd.concat(results, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14f81a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è File data baru tidak ditemukan. Menggunakan histori yang ada saja.\n",
      "Epoch 1/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - loss: 0.8262 - mae: 0.7272 - val_loss: 0.4783 - val_mae: 0.5215 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.5071 - mae: 0.5574 - val_loss: 0.4223 - val_mae: 0.4668 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 0.4706 - mae: 0.5316 - val_loss: 0.4178 - val_mae: 0.4619 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.4562 - mae: 0.5187 - val_loss: 0.4406 - val_mae: 0.5044 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.4498 - mae: 0.5123 - val_loss: 0.4078 - val_mae: 0.4382 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.4414 - mae: 0.5039 - val_loss: 0.4104 - val_mae: 0.4452 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.4376 - mae: 0.4999 - val_loss: 0.4019 - val_mae: 0.4107 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.4339 - mae: 0.4960 - val_loss: 0.4050 - val_mae: 0.4262 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.4307 - mae: 0.4923 - val_loss: 0.3999 - val_mae: 0.4045 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.4297 - mae: 0.4918 - val_loss: 0.4063 - val_mae: 0.4304 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.4266 - mae: 0.4885 - val_loss: 0.4053 - val_mae: 0.4236 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.4246 - mae: 0.4863 - val_loss: 0.4019 - val_mae: 0.4103 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.4243 - mae: 0.4860 - val_loss: 0.4055 - val_mae: 0.4282 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.4226 - mae: 0.4843 - val_loss: 0.3998 - val_mae: 0.4035 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.4221 - mae: 0.4821 - val_loss: 0.4060 - val_mae: 0.4303 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.4202 - mae: 0.4813 - val_loss: 0.4003 - val_mae: 0.4021 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.4190 - mae: 0.4789 - val_loss: 0.4027 - val_mae: 0.4165 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.4186 - mae: 0.4783 - val_loss: 0.4011 - val_mae: 0.4060 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 0.4170 - mae: 0.4782 - val_loss: 0.4043 - val_mae: 0.4159 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.4158 - mae: 0.4773 - val_loss: 0.4018 - val_mae: 0.4080 - learning_rate: 5.0000e-04\n",
      "üîπ RF models trained: 3438\n",
      "‚úÖ Prediksi tersimpan di prediksi_28_09_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Pipeline main\n",
    "# ==============================\n",
    "historical_file = \"data_historis.csv\"\n",
    "file_new_data = \"data_29_09_2025.csv\"\n",
    "\n",
    "# Load histori\n",
    "historical_df = pd.read_csv(historical_file) if Path(historical_file).exists() else pd.DataFrame()\n",
    "# Load data baru\n",
    "new_df = pd.read_csv(file_new_data) if Path(file_new_data).exists() else pd.DataFrame()\n",
    "\n",
    "# Pastikan kolom ada walau kosong\n",
    "for df in [historical_df,new_df]:\n",
    "    for col in [\"request_date\",\"origin_geo_hash\",\"time_slot\",\"request_count\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "# Gabung data baru ke histori\n",
    "if not new_df.empty:\n",
    "    historical_df = pd.concat([historical_df,new_df], ignore_index=True)\n",
    "    historical_df.to_csv(historical_file, index=False)\n",
    "    print(f\"‚úÖ Data baru ditambahkan. Total rows: {len(historical_df)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è File data baru tidak ditemukan. Menggunakan histori yang ada saja.\")\n",
    "\n",
    "# Inisialisasi predictor\n",
    "predictor = TimeSeriesRequestPredictor30MinHybrid()\n",
    "\n",
    "# Preprocess\n",
    "df_full = predictor.load_and_preprocess_with_new_geos(historical_df, new_df)\n",
    "\n",
    "# Training\n",
    "train_data,_ = predictor.train(df=df_full, epochs=20)\n",
    "\n",
    "# Ambil tanggal terakhir & fill missing slot\n",
    "max_date = pd.to_datetime(df_full[\"request_date\"]).max()\n",
    "df_last_day = df_full[df_full[\"request_date\"]==max_date].copy()\n",
    "df_last_day = predictor.fill_missing_slots(df_last_day)\n",
    "\n",
    "# Prediksi semua geo untuk H+1\n",
    "df_pred = predictor.predict_all_next_day_30min_filtered(df_last_day)\n",
    "df_pred[\"date\"] = pd.to_datetime(df_pred[\"date\"]).dt.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "# Simpan prediksi\n",
    "prediksi_file = f\"prediksi_{max_date.strftime('%d_%m_%Y')}.csv\"\n",
    "df_pred.to_csv(prediksi_file, index=False)\n",
    "print(f\"‚úÖ Prediksi tersimpan di {prediksi_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9a126b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(pred_file: str, actual_file: str, output_file=\"evaluasi_geo_hash_sama.csv\"):\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "    # --- Baca file prediksi ---\n",
    "    if not Path(pred_file).exists():\n",
    "        print(f\"‚ö†Ô∏è File {pred_file} tidak ditemukan.\")\n",
    "        return\n",
    "    df_pred = pd.read_csv(pred_file)\n",
    "    df_pred[\"date\"] = pd.to_datetime(df_pred[\"date\"], dayfirst=True)\n",
    "\n",
    "    # --- Baca file actual ---\n",
    "    if not Path(actual_file).exists():\n",
    "        print(f\"‚ö†Ô∏è File {actual_file} tidak ditemukan.\")\n",
    "        return\n",
    "    df_actual = pd.read_csv(actual_file)\n",
    "\n",
    "    # Buat slot_30min dari time_slot\n",
    "    def extract_slot(val):\n",
    "        try:\n",
    "            h, m = map(int, val.split(\"-\")[0].strip().split(\":\"))\n",
    "            return h * 2 + (m // 30)\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    df_actual = pd.read_csv(actual_file, skipinitialspace=True)\n",
    "    df_actual.columns = df_actual.columns.str.strip()  # hapus spasi\n",
    "    if \"request_date\" not in df_actual.columns:\n",
    "        print(\"‚ö†Ô∏è Kolom 'request_date' tidak ditemukan di CSV actual!\")\n",
    "        return\n",
    "\n",
    "    df_actual[\"slot_30min\"] = df_actual[\"time_slot\"].apply(extract_slot)\n",
    "    df_actual[\"geo_hash\"] = df_actual[\"origin_geo_hash\"]\n",
    "    df_actual[\"date\"] = pd.to_datetime(df_actual[\"request_date\"], dayfirst=True)\n",
    "    df_actual[\"actual_request_count\"] = df_actual[\"request_count\"]\n",
    "\n",
    "    # Pilih kolom penting\n",
    "    df_actual_proc = df_actual[[\"geo_hash\",\"date\",\"slot_30min\",\"actual_request_count\"]]\n",
    "\n",
    "    # Filter hanya geo_hash yang ada di prediksi\n",
    "    valid_geo = df_pred[\"geo_hash\"].unique()\n",
    "    df_actual_proc = df_actual_proc[df_actual_proc[\"geo_hash\"].isin(valid_geo)]\n",
    "\n",
    "    # Merge inner\n",
    "    merged = pd.merge(\n",
    "        df_pred,\n",
    "        df_actual_proc,\n",
    "        on=[\"geo_hash\",\"date\",\"slot_30min\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    if not merged.empty:\n",
    "        mae = mean_absolute_error(merged[\"actual_request_count\"], merged[\"predicted_request_count\"])\n",
    "        r2  = r2_score(merged[\"actual_request_count\"], merged[\"predicted_request_count\"])\n",
    "        rmse = sqrt(mean_squared_error(merged[\"actual_request_count\"], merged[\"predicted_request_count\"]))\n",
    "        print(f\"üìä Evaluasi (geo_hash sama): MAE={mae:.4f}, R¬≤={r2:.4f}, RMSE={rmse:.4f}, N={len(merged)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Tidak ada slot/geo_hash yang cocok untuk evaluasi.\")\n",
    "\n",
    "    # Simpan hasil evaluasi\n",
    "    merged.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Hasil evaluasi disimpan di {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af83e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluasi (geo_hash sama): MAE=3.0974, R¬≤=-0.6117, RMSE=4.5352, N=1083\n",
      "‚úÖ Hasil evaluasi disimpan di evaluasi_manual.csv\n"
     ]
    }
   ],
   "source": [
    "evaluate_prediction(\n",
    "    pred_file=\"prediksi_29_09_2025.csv\",\n",
    "    actual_file=\"data_29_09_202.csv\",\n",
    "    output_file=\"evaluasi_manual.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
