{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d402acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "583e0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimeSeriesRequestPredictor:\n",
    "    def __init__(self, sequence_length=7, prediction_horizon=1):\n",
    "        \"\"\"\n",
    "        Pipeline prediksi request count dengan LSTM\n",
    "        Args:\n",
    "            sequence_length: panjang window historis (hari)\n",
    "            prediction_horizon: horizon prediksi (berapa hari ke depan)\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.scaler = StandardScaler()\n",
    "        self.geo_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def load_and_preprocess_data(self, csv_file_path):\n",
    "        \"\"\"\n",
    "        Load dan preprocess data CSV\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Pastikan request_date datetime\n",
    "        df[\"request_date\"] = pd.to_datetime(df[\"request_date\"], errors=\"coerce\")\n",
    "\n",
    "        # Ekstrak jam dari kolom time_slot (jika ada)\n",
    "        if \"time_slot\" in df.columns:\n",
    "            def extract_hour(val):\n",
    "                if pd.isna(val):\n",
    "                    return 0\n",
    "                if isinstance(val, str) and \"-\" in val:\n",
    "                    return int(val.split(\":\")[0])\n",
    "                try:\n",
    "                    return pd.to_datetime(val, errors=\"coerce\").hour\n",
    "                except Exception:\n",
    "                    return 0\n",
    "\n",
    "            df[\"time_slot\"] = df[\"time_slot\"].apply(extract_hour)\n",
    "        else:\n",
    "            df[\"time_slot\"] = 0\n",
    "\n",
    "        # Aggregate per hari per lokasi\n",
    "        daily_data = df.groupby([\"request_date\", \"origin_geo_hash\"]).agg({\n",
    "            \"request_count\": \"sum\",\n",
    "            \"time_slot\": \"count\"  # jumlah slot aktif\n",
    "        }).reset_index()\n",
    "\n",
    "        daily_data.rename(columns={\"time_slot\": \"active_slots\"}, inplace=True)\n",
    "\n",
    "        # Feature engineering\n",
    "        daily_data[\"day_of_week\"] = daily_data[\"request_date\"].dt.dayofweek\n",
    "        daily_data[\"day_of_month\"] = daily_data[\"request_date\"].dt.day\n",
    "        daily_data[\"month\"] = daily_data[\"request_date\"].dt.month\n",
    "        daily_data[\"is_weekend\"] = (daily_data[\"day_of_week\"] >= 5).astype(int)\n",
    "\n",
    "        # Encode geo hash\n",
    "        daily_data[\"geo_encoded\"] = self.geo_encoder.fit_transform(daily_data[\"origin_geo_hash\"])\n",
    "\n",
    "        # Sort by date and geo\n",
    "        daily_data = daily_data.sort_values([\"origin_geo_hash\", \"request_date\"])\n",
    "\n",
    "        return daily_data\n",
    "\n",
    "    def create_sequences(self, data):\n",
    "        \"\"\"\n",
    "        Buat sequences untuk LSTM\n",
    "        \"\"\"\n",
    "        sequences, targets = [], []\n",
    "        features = [\n",
    "            \"request_count\",\n",
    "            \"active_slots\",\n",
    "            \"day_of_week\",\n",
    "            \"day_of_month\",\n",
    "            \"month\",\n",
    "            \"is_weekend\",\n",
    "            \"geo_encoded\",\n",
    "        ]\n",
    "\n",
    "        for geo_hash in data[\"origin_geo_hash\"].unique():\n",
    "            geo_data = data[data[\"origin_geo_hash\"] == geo_hash].sort_values(\n",
    "                \"request_date\"\n",
    "            )\n",
    "\n",
    "            # Lengkapi missing dates\n",
    "            date_range = pd.date_range(\n",
    "                start=geo_data[\"request_date\"].min(),\n",
    "                end=geo_data[\"request_date\"].max(),\n",
    "                freq=\"D\",\n",
    "            )\n",
    "            geo_data = (\n",
    "                geo_data.set_index(\"request_date\")\n",
    "                .reindex(date_range, fill_value=0)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"request_date\"})\n",
    "            )\n",
    "\n",
    "            feature_data = geo_data[features].values\n",
    "\n",
    "            for i in range(len(feature_data) - self.sequence_length):\n",
    "                if (\n",
    "                    i + self.sequence_length + self.prediction_horizon\n",
    "                    <= len(feature_data)\n",
    "                ):\n",
    "                    seq = feature_data[i : i + self.sequence_length]\n",
    "                    target = feature_data[\n",
    "                        i + self.sequence_length : i + self.sequence_length\n",
    "                        + self.prediction_horizon,\n",
    "                        0,\n",
    "                    ]\n",
    "                    sequences.append(seq)\n",
    "                    targets.append(target)\n",
    "\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Build LSTM model\n",
    "        \"\"\"\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Input(shape=input_shape),\n",
    "                LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "                LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "                LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "                Dense(64, activation=\"relu\"),\n",
    "                Dropout(0.3),\n",
    "                Dense(32, activation=\"relu\"),\n",
    "                Dropout(0.2),\n",
    "                Dense(self.prediction_horizon, activation=\"linear\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        model.compile(optimizer=Adam(0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "        return model\n",
    "\n",
    "    def temporal_split(self, X, y, val_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Split data secara temporal (bukan random)\n",
    "        \"\"\"\n",
    "        split_index = int(len(X) * (1 - val_ratio))\n",
    "        X_train, X_val = X[:split_index], X[split_index:]\n",
    "        y_train, y_val = y[:split_index], y[split_index:]\n",
    "        return X_train, X_val, y_train, y_val\n",
    "\n",
    "    def train(self, csv_file_path, validation_split=0.2, epochs=50):\n",
    "        \"\"\"\n",
    "        Train model dengan temporal split\n",
    "        \"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        data = self.load_and_preprocess_data(csv_file_path)\n",
    "\n",
    "        print(\"Creating sequences...\")\n",
    "        X, y = self.create_sequences(data)\n",
    "\n",
    "        print(f\"Total sequences: {len(X)}\")\n",
    "        print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        # Scale\n",
    "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "        X_scaled = self.scaler.fit_transform(X_reshaped)\n",
    "        X_scaled = X_scaled.reshape(X.shape)\n",
    "\n",
    "        # Temporal split\n",
    "        X_train, X_val, y_train, y_val = self.temporal_split(X_scaled, y, validation_split)\n",
    "\n",
    "        # Build model\n",
    "        self.model = self.build_model((X.shape[1], X.shape[2]))\n",
    "        print(self.model.summary())\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6),\n",
    "        ]\n",
    "\n",
    "        history = self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        return data, history\n",
    "\n",
    "    def predict_next_day(self, data, geo_hash):\n",
    "        \"\"\"\n",
    "        Prediksi request count untuk besok di 1 lokasi (geo_hash)\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model belum di-train!\")\n",
    "\n",
    "        # Ambil data terakhir\n",
    "        geo_data = data[data[\"origin_geo_hash\"] == geo_hash].sort_values(\"request_date\")\n",
    "        features = [\n",
    "            \"request_count\", \"active_slots\", \"day_of_week\",\n",
    "            \"day_of_month\", \"month\", \"is_weekend\", \"geo_encoded\"\n",
    "        ]\n",
    "        \n",
    "        # Cek apakah ada cukup data untuk panjang urutan yang ditentukan\n",
    "        if len(geo_data) < self.sequence_length:\n",
    "            raise ValueError(f\"Tidak cukup data untuk geo_hash '{geo_hash}'. Dibutuhkan {self.sequence_length} hari, tapi hanya tersedia {len(geo_data)} hari.\")\n",
    "            \n",
    "        last_sequence = geo_data[features].tail(self.sequence_length).values\n",
    "\n",
    "        # Normalisasi: perbaikan di sini untuk mengatasi ValueError\n",
    "        seq_scaled = self.scaler.transform(last_sequence)\n",
    "        seq_scaled = seq_scaled.reshape(1, self.sequence_length, len(features))\n",
    "\n",
    "        # Prediksi\n",
    "        pred = self.model.predict(seq_scaled, verbose=0)[0]\n",
    "\n",
    "        # Tentukan tanggal besok\n",
    "        tomorrow = geo_data[\"request_date\"].max() + timedelta(days=1)\n",
    "\n",
    "        # Kembalikan sebagai tabel (DataFrame)\n",
    "        result_df = pd.DataFrame({\n",
    "            \"geo_hash\": [geo_hash],\n",
    "            \"date\": [tomorrow],\n",
    "            \"predicted_request_count\": [max(0, pred[0])] # Jaga jangan negatif\n",
    "        })\n",
    "        return result_df\n",
    "    \n",
    "    def predict_all_next_day(self, data):\n",
    "        \"\"\"\n",
    "        Prediksi request_count besok untuk semua lokasi (geo_hash)\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        tomorrow = data[\"request_date\"].max() + timedelta(days=1)\n",
    "\n",
    "        for geo_hash in data[\"origin_geo_hash\"].unique():\n",
    "            try:\n",
    "                pred_df = self.predict_next_day(data, geo_hash)\n",
    "                results.append(pred_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {geo_hash}, error: {e}\")\n",
    "\n",
    "        if results:\n",
    "            return pd.concat(results, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame(columns=[\"geo_hash\", \"date\", \"predicted_request_count\"])\n",
    "\n",
    "    def evaluate_yesterday_prediction(self, data, yesterday_predictions):\n",
    "        \"\"\"\n",
    "        Evaluasi prediksi kemarin dengan data aktual hari ini\n",
    "        Args:\n",
    "            data: Dataframe hasil preprocessing (yang sudah ada data terbaru)\n",
    "            yesterday_predictions: DataFrame hasil prediksi kemarin (geo_hash, date, predicted_request_count)\n",
    "        \"\"\"\n",
    "        # Ambil data aktual di tanggal yang sama\n",
    "        actuals = data[data[\"request_date\"].isin(yesterday_predictions[\"date\"])][\n",
    "            [\"origin_geo_hash\", \"request_date\", \"request_count\"]\n",
    "        ]\n",
    "        actuals = actuals.rename(columns={\n",
    "            \"origin_geo_hash\": \"geo_hash\",\n",
    "            \"request_date\": \"date\",\n",
    "            \"request_count\": \"actual_request_count\"\n",
    "        })\n",
    "\n",
    "        # Merge prediksi dengan aktual\n",
    "        merged = pd.merge(yesterday_predictions, actuals, on=[\"geo_hash\", \"date\"], how=\"inner\")\n",
    "\n",
    "        if merged.empty:\n",
    "            print(\"⚠️ Tidak ada data aktual untuk dievaluasi.\")\n",
    "            return None\n",
    "\n",
    "        # Hitung metrik\n",
    "        mae = mean_absolute_error(merged[\"actual_request_count\"], merged[\"predicted_request_count\"])\n",
    "        rmse = mean_squared_error(merged[\"actual_request_count\"], merged[\"predicted_request_count\"], squared=False)\n",
    "\n",
    "        print(f\"Evaluasi untuk {merged['date'].iloc[0].date()}:\")\n",
    "        print(f\"  MAE  = {mae:.2f}\")\n",
    "        print(f\"  RMSE = {rmse:.2f}\")\n",
    "\n",
    "        return merged, {\"MAE\": mae, \"RMSE\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c9f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Creating sequences...\n",
      "Total sequences: 2064\n",
      "X shape: (2064, 7, 7), y shape: (2064, 1)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 7, 128)            69632     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 7, 64)             49408     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135681 (530.00 KB)\n",
      "Trainable params: 135681 (530.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "52/52 [==============================] - 7s 22ms/step - loss: 167.4198 - mae: 8.8224 - val_loss: 223.0824 - val_mae: 10.3003 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 106.7181 - mae: 7.4859 - val_loss: 227.9863 - val_mae: 10.0279 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 105.0729 - mae: 7.3515 - val_loss: 209.1559 - val_mae: 9.6456 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 99.4160 - mae: 7.2623 - val_loss: 206.4132 - val_mae: 9.2289 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 98.0141 - mae: 7.1326 - val_loss: 184.3676 - val_mae: 9.3128 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 97.9835 - mae: 7.0395 - val_loss: 178.5914 - val_mae: 9.1992 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 97.1091 - mae: 7.0723 - val_loss: 170.3051 - val_mae: 8.7759 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 96.4192 - mae: 7.0377 - val_loss: 169.3702 - val_mae: 8.6966 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 94.8924 - mae: 6.9557 - val_loss: 171.5844 - val_mae: 8.8713 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 94.8327 - mae: 7.0159 - val_loss: 172.4180 - val_mae: 8.8996 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 95.0733 - mae: 6.9975 - val_loss: 170.0957 - val_mae: 8.7833 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 95.5365 - mae: 7.0778 - val_loss: 166.1729 - val_mae: 8.7872 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 91.7755 - mae: 6.8492 - val_loss: 162.6517 - val_mae: 8.8618 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 99.2355 - mae: 7.0480 - val_loss: 170.1881 - val_mae: 8.6608 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 96.1189 - mae: 7.0692 - val_loss: 163.6720 - val_mae: 8.5755 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 92.2082 - mae: 6.8493 - val_loss: 157.5984 - val_mae: 8.5980 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 95.7287 - mae: 7.0065 - val_loss: 157.7737 - val_mae: 8.8221 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 92.8226 - mae: 6.9887 - val_loss: 152.6955 - val_mae: 8.5329 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 93.3642 - mae: 6.9856 - val_loss: 155.6167 - val_mae: 8.4215 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 94.2258 - mae: 6.9107 - val_loss: 150.9407 - val_mae: 8.5975 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 92.8271 - mae: 6.9574 - val_loss: 152.4577 - val_mae: 8.4694 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 91.1572 - mae: 6.9110 - val_loss: 152.1602 - val_mae: 8.2521 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 93.2300 - mae: 6.8944 - val_loss: 155.7830 - val_mae: 8.2759 - lr: 0.0010\n",
      "Epoch 24/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 93.1287 - mae: 6.8426 - val_loss: 149.7072 - val_mae: 8.4483 - lr: 0.0010\n",
      "Epoch 25/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 91.1912 - mae: 6.8577 - val_loss: 142.5812 - val_mae: 8.5006 - lr: 0.0010\n",
      "Epoch 26/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 92.1955 - mae: 6.8831 - val_loss: 147.1859 - val_mae: 8.2906 - lr: 0.0010\n",
      "Epoch 27/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 91.1788 - mae: 6.8433 - val_loss: 145.0254 - val_mae: 8.6580 - lr: 0.0010\n",
      "Epoch 28/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 90.6314 - mae: 6.7927 - val_loss: 143.6152 - val_mae: 8.5826 - lr: 0.0010\n",
      "Epoch 29/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 90.2798 - mae: 6.9108 - val_loss: 145.5797 - val_mae: 8.3048 - lr: 0.0010\n",
      "Epoch 30/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 89.2450 - mae: 6.8309 - val_loss: 142.0549 - val_mae: 8.2517 - lr: 0.0010\n",
      "Skipping qqgeut, error: Tidak cukup data untuk geo_hash 'qqgeut'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqggyc, error: Tidak cukup data untuk geo_hash 'qqggyc'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqgu4s, error: Tidak cukup data untuk geo_hash 'qqgu4s'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqgudx, error: Tidak cukup data untuk geo_hash 'qqgudx'. Dibutuhkan 7 hari, tapi hanya tersedia 3 hari.\n",
      "Skipping qqgug5, error: Tidak cukup data untuk geo_hash 'qqgug5'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguk9, error: Tidak cukup data untuk geo_hash 'qqguk9'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqgumg, error: Tidak cukup data untuk geo_hash 'qqgumg'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqgunw, error: Tidak cukup data untuk geo_hash 'qqgunw'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqguqp, error: Tidak cukup data untuk geo_hash 'qqguqp'. Dibutuhkan 7 hari, tapi hanya tersedia 3 hari.\n",
      "Skipping qqgurz, error: Tidak cukup data untuk geo_hash 'qqgurz'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqguuc, error: Tidak cukup data untuk geo_hash 'qqguuc'. Dibutuhkan 7 hari, tapi hanya tersedia 5 hari.\n",
      "Skipping qqguw9, error: Tidak cukup data untuk geo_hash 'qqguw9'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguwg, error: Tidak cukup data untuk geo_hash 'qqguwg'. Dibutuhkan 7 hari, tapi hanya tersedia 3 hari.\n",
      "Skipping qqguwr, error: Tidak cukup data untuk geo_hash 'qqguwr'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguws, error: Tidak cukup data untuk geo_hash 'qqguws'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguwu, error: Tidak cukup data untuk geo_hash 'qqguwu'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqguww, error: Tidak cukup data untuk geo_hash 'qqguww'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping qqguwz, error: Tidak cukup data untuk geo_hash 'qqguwz'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqgux6, error: Tidak cukup data untuk geo_hash 'qqgux6'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping qqguxh, error: Tidak cukup data untuk geo_hash 'qqguxh'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguxr, error: Tidak cukup data untuk geo_hash 'qqguxr'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguyd, error: Tidak cukup data untuk geo_hash 'qqguyd'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqguyu, error: Tidak cukup data untuk geo_hash 'qqguyu'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguz0, error: Tidak cukup data untuk geo_hash 'qqguz0'. Dibutuhkan 7 hari, tapi hanya tersedia 5 hari.\n",
      "Skipping qqguzb, error: Tidak cukup data untuk geo_hash 'qqguzb'. Dibutuhkan 7 hari, tapi hanya tersedia 5 hari.\n",
      "Skipping qqguzg, error: Tidak cukup data untuk geo_hash 'qqguzg'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqguzv, error: Tidak cukup data untuk geo_hash 'qqguzv'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqgvhz, error: Tidak cukup data untuk geo_hash 'qqgvhz'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqgvnr, error: Tidak cukup data untuk geo_hash 'qqgvnr'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqgvp3, error: Tidak cukup data untuk geo_hash 'qqgvp3'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqtn3d, error: Tidak cukup data untuk geo_hash 'qqtn3d'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqttv8, error: Tidak cukup data untuk geo_hash 'qqttv8'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqu486, error: Tidak cukup data untuk geo_hash 'qqu486'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping qqu5bh, error: Tidak cukup data untuk geo_hash 'qqu5bh'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqu8b6, error: Tidak cukup data untuk geo_hash 'qqu8b6'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qquh4v, error: Tidak cukup data untuk geo_hash 'qquh4v'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qquh8r, error: Tidak cukup data untuk geo_hash 'qquh8r'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qquj03, error: Tidak cukup data untuk geo_hash 'qquj03'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qquj0x, error: Tidak cukup data untuk geo_hash 'qquj0x'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqvbu2, error: Tidak cukup data untuk geo_hash 'qqvbu2'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqww69, error: Tidak cukup data untuk geo_hash 'qqww69'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqwx2g, error: Tidak cukup data untuk geo_hash 'qqwx2g'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqy08y, error: Tidak cukup data untuk geo_hash 'qqy08y'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqy2n8, error: Tidak cukup data untuk geo_hash 'qqy2n8'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qw3ytk, error: Tidak cukup data untuk geo_hash 'qw3ytk'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qw3yug, error: Tidak cukup data untuk geo_hash 'qw3yug'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qw3ywj, error: Tidak cukup data untuk geo_hash 'qw3ywj'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qw84f6, error: Tidak cukup data untuk geo_hash 'qw84f6'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qxg0gp, error: Tidak cukup data untuk geo_hash 'qxg0gp'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping qxhfwj, error: Tidak cukup data untuk geo_hash 'qxhfwj'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping w21yq4, error: Tidak cukup data untuk geo_hash 'w21yq4'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping w24m4p, error: Tidak cukup data untuk geo_hash 'w24m4p'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping w8589q, error: Tidak cukup data untuk geo_hash 'w8589q'. Dibutuhkan 7 hari, tapi hanya tersedia 5 hari.\n",
      "Skipping w858c1, error: Tidak cukup data untuk geo_hash 'w858c1'. Dibutuhkan 7 hari, tapi hanya tersedia 3 hari.\n",
      "Prediksi kemarin:\n",
      "    geo_hash       date  predicted_request_count\n",
      "0     qpz6e8 2025-09-25                 6.240051\n",
      "1     qqgb0x 2025-09-26                11.278461\n",
      "2     qqgfmy 2025-09-26                 8.971997\n",
      "3     qqgfps 2025-09-26                 6.936623\n",
      "4     qqgfqd 2025-09-26                 8.198394\n",
      "..       ...        ...                      ...\n",
      "281   w0whm4 2025-09-26                14.846626\n",
      "282   w0wht9 2025-09-26                21.206566\n",
      "283   w204gg 2025-09-26                 5.768125\n",
      "284   w204ut 2025-09-24                 8.504452\n",
      "285   w21yrk 2025-09-26                11.412193\n",
      "\n",
      "[286 rows x 3 columns]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './studio_results_20250925_1927.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(yesterday_predictions)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ==== Keesokan harinya kamu sudah punya data baru (tgl 8 masuk) ====\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load ulang data dengan update\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m new_data = \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./studio_results_20250925_1927.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Evaluasi hasil prediksi kemarin dengan data aktual tgl 8\u001b[39;00m\n\u001b[32m     17\u001b[39m eval_results, metrics = predictor.evaluate_yesterday_prediction(new_data, yesterday_predictions)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mTimeSeriesRequestPredictor.load_and_preprocess_data\u001b[39m\u001b[34m(self, csv_file_path)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_preprocess_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_file_path):\n\u001b[32m     16\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    Load dan preprocess data CSV\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Pastikan request_date datetime\u001b[39;00m\n\u001b[32m     22\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mrequest_date\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(df[\u001b[33m\"\u001b[39m\u001b[33mrequest_date\u001b[39m\u001b[33m\"\u001b[39m], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    935\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    936\u001b[39m     dialect,\n\u001b[32m    937\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    945\u001b[39m )\n\u001b[32m    946\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    608\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1447\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1704\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1716\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    862\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    872\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './studio_results_20250925_1927.csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    predictor = TimeSeriesRequestPredictor(sequence_length=7, prediction_horizon=1)\n",
    "\n",
    "    # Train pertama kali\n",
    "    data, history = predictor.train(\"./studio_results_20250925_1926.csv\", epochs=30)\n",
    "\n",
    "    # Prediksi untuk besok (misal tanggal 8)\n",
    "    yesterday_predictions = predictor.predict_all_next_day(data)\n",
    "    print(\"Prediksi kemarin:\")\n",
    "    print(yesterday_predictions)\n",
    "\n",
    "    # ==== Keesokan harinya kamu sudah punya data baru (tgl 8 masuk) ====\n",
    "    # Load ulang data dengan update\n",
    "    new_data = predictor.load_and_preprocess_data(\"./New_Data.csv\")\n",
    "\n",
    "    # Evaluasi hasil prediksi kemarin dengan data aktual tgl 8\n",
    "    eval_results, metrics = predictor.evaluate_yesterday_prediction(new_data, yesterday_predictions)\n",
    "    print(eval_results)\n",
    "\n",
    "    # Sekarang bikin prediksi lagi untuk tgl 9\n",
    "    today_predictions = predictor.predict_all_next_day(new_data)\n",
    "    print(\"Prediksi hari ini:\")\n",
    "    print(today_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
