{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d402acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583e0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesRequestPredictor:\n",
    "    def __init__(self, sequence_length=7, prediction_horizon=1):\n",
    "        \"\"\"\n",
    "        Pipeline untuk prediksi request count menggunakan LSTM\n",
    "        \n",
    "        Args:\n",
    "            sequence_length: Jumlah hari historis untuk prediksi (window size)\n",
    "            prediction_horizon: Jumlah hari ke depan yang diprediksi\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.scaler = StandardScaler()\n",
    "        self.geo_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.feature_columns = []\n",
    "        \n",
    "    def load_and_preprocess_data(self, csv_file_path):\n",
    "        \"\"\"\n",
    "        Load dan preprocess data CSV\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Convert datetime\n",
    "        df['request_date'] = pd.to_datetime(df['request_date'])\n",
    "        df['time_slot'] = pd.to_datetime(df['time_slot'], format='%H:%M-%H:%M', errors='coerce').dt.hour\n",
    "        \n",
    "        # Aggregate per hari per lokasi\n",
    "        daily_data = df.groupby(['request_date', 'origin_geo_hash']).agg({\n",
    "            'request_count': 'sum',\n",
    "            'time_slot': 'count'  # sebagai feature tambahan (jumlah time slot aktif)\n",
    "        }).reset_index()\n",
    "        \n",
    "        daily_data.rename(columns={'time_slot': 'active_slots'}, inplace=True)\n",
    "        \n",
    "        # Feature engineering\n",
    "        daily_data['day_of_week'] = daily_data['request_date'].dt.dayofweek\n",
    "        daily_data['day_of_month'] = daily_data['request_date'].dt.day\n",
    "        daily_data['month'] = daily_data['request_date'].dt.month\n",
    "        daily_data['is_weekend'] = (daily_data['day_of_week'] >= 5).astype(int)\n",
    "        \n",
    "        # Encode geo hash\n",
    "        daily_data['geo_encoded'] = self.geo_encoder.fit_transform(daily_data['origin_geo_hash'])\n",
    "        \n",
    "        # Sort by date and geo\n",
    "        daily_data = daily_data.sort_values(['origin_geo_hash', 'request_date'])\n",
    "        \n",
    "        return daily_data\n",
    "    \n",
    "    def create_sequences(self, data):\n",
    "        \"\"\"\n",
    "        Buat sequences untuk LSTM training\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        \n",
    "        # Group by geo hash\n",
    "        for geo_hash in data['origin_geo_hash'].unique():\n",
    "            geo_data = data[data['origin_geo_hash'] == geo_hash].sort_values('request_date')\n",
    "            \n",
    "            # Pastikan data continuous (fill missing dates)\n",
    "            date_range = pd.date_range(\n",
    "                start=geo_data['request_date'].min(),\n",
    "                end=geo_data['request_date'].max(),\n",
    "                freq='D'\n",
    "            )\n",
    "            \n",
    "            # Reindex dengan semua tanggal\n",
    "            geo_data = geo_data.set_index('request_date').reindex(date_range, fill_value=0)\n",
    "            geo_data.index.name = 'request_date'\n",
    "            geo_data = geo_data.reset_index()\n",
    "            \n",
    "            # Fill missing values dengan forward fill atau interpolation\n",
    "            geo_data = geo_data.fillna(method='ffill').fillna(method='bfill')\n",
    "            \n",
    "            # Features untuk model\n",
    "            features = [\n",
    "                'request_count', 'active_slots', 'day_of_week', \n",
    "                'day_of_month', 'month', 'is_weekend', 'geo_encoded'\n",
    "            ]\n",
    "            \n",
    "            feature_data = geo_data[features].values\n",
    "            \n",
    "            # Create sequences\n",
    "            for i in range(len(feature_data) - self.sequence_length):\n",
    "                if i + self.sequence_length + self.prediction_horizon <= len(feature_data):\n",
    "                    # Input sequence\n",
    "                    seq = feature_data[i:i + self.sequence_length]\n",
    "                    # Target (request_count saja)\n",
    "                    target = feature_data[i + self.sequence_length:i + self.sequence_length + self.prediction_horizon, 0]\n",
    "                    \n",
    "                    sequences.append(seq)\n",
    "                    targets.append(target)\n",
    "        \n",
    "        return np.array(sequences), np.array(targets)\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Build LSTM model\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            \n",
    "            # LSTM layers\n",
    "            LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "            \n",
    "            # Dense layers\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(self.prediction_horizon, activation='linear')  # Linear untuk regression\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, csv_file_path, validation_split=0.2, epochs=100):\n",
    "        \"\"\"\n",
    "        Train model\n",
    "        \"\"\"\n",
    "        # Load dan preprocess data\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        data = self.load_and_preprocess_data(csv_file_path)\n",
    "        \n",
    "        # Create sequences\n",
    "        print(\"Creating sequences...\")\n",
    "        X, y = self.create_sequences(data)\n",
    "        \n",
    "        print(f\"Total sequences: {len(X)}\")\n",
    "        print(f\"Input shape: {X.shape}\")\n",
    "        print(f\"Target shape: {y.shape}\")\n",
    "        \n",
    "        # Normalize features\n",
    "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "        X_scaled = self.scaler.fit_transform(X_reshaped)\n",
    "        X_scaled = X_scaled.reshape(X.shape)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_scaled, y, test_size=validation_split, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model((X.shape[1], X.shape[2]))\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=15, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=7, min_lr=1e-7)\n",
    "        ]\n",
    "        \n",
    "        # Train\n",
    "        print(\"Training model...\")\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, data, geo_hash, start_date, days_ahead=7):\n",
    "        \"\"\"\n",
    "        Prediksi untuk geo hash tertentu\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model belum di-train!\")\n",
    "        \n",
    "        # Filter data untuk geo hash tertentu\n",
    "        geo_data = data[data['origin_geo_hash'] == geo_hash].sort_values('request_date')\n",
    "        \n",
    "        # Ambil sequence terakhir\n",
    "        features = [\n",
    "            'request_count', 'active_slots', 'day_of_week', \n",
    "            'day_of_month', 'month', 'is_weekend', 'geo_encoded'\n",
    "        ]\n",
    "        \n",
    "        last_sequence = geo_data[features].tail(self.sequence_length).values\n",
    "        \n",
    "        predictions = []\n",
    "        current_sequence = last_sequence.copy()\n",
    "        \n",
    "        for i in range(days_ahead):\n",
    "            # Normalize sequence\n",
    "            seq_reshaped = current_sequence.reshape(1, -1)\n",
    "            seq_scaled = self.scaler.transform(seq_reshaped)\n",
    "            seq_scaled = seq_scaled.reshape(1, self.sequence_length, len(features))\n",
    "            \n",
    "            # Predict\n",
    "            pred = self.model.predict(seq_scaled, verbose=0)[0]\n",
    "            predictions.extend(pred)\n",
    "            \n",
    "            # Update sequence untuk prediksi berikutnya\n",
    "            if i < days_ahead - 1:\n",
    "                next_date = start_date + timedelta(days=i+1)\n",
    "                \n",
    "                # Create new row dengan prediksi\n",
    "                new_row = current_sequence[-1].copy()\n",
    "                new_row[0] = pred[0]  # request_count\n",
    "                new_row[2] = next_date.weekday()  # day_of_week\n",
    "                new_row[3] = next_date.day  # day_of_month\n",
    "                new_row[4] = next_date.month  # month\n",
    "                new_row[5] = 1 if next_date.weekday() >= 5 else 0  # is_weekend\n",
    "                \n",
    "                # Slide sequence\n",
    "                current_sequence = np.vstack([current_sequence[1:], new_row.reshape(1, -1)])\n",
    "        \n",
    "        return predictions[:days_ahead]\n",
    "    \n",
    "    def plot_predictions(self, actual_data, predictions, geo_hash, start_date, days_ahead):\n",
    "        \"\"\"\n",
    "        Plot hasil prediksi\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Historical data\n",
    "        geo_data = actual_data[actual_data['origin_geo_hash'] == geo_hash].sort_values('request_date')\n",
    "        historical_dates = geo_data['request_date'].tail(30)\n",
    "        historical_counts = geo_data['request_count'].tail(30)\n",
    "        \n",
    "        plt.plot(historical_dates, historical_counts, label='Historical', marker='o')\n",
    "        \n",
    "        # Predictions\n",
    "        pred_dates = [start_date + timedelta(days=i) for i in range(days_ahead)]\n",
    "        plt.plot(pred_dates, predictions, label='Predictions', marker='s', color='red')\n",
    "        \n",
    "        plt.title(f'Request Count Prediction - {geo_hash}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Request Count')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac1684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesRequestPredictor:\n",
    "    def __init__(self, n_past=24, n_future=24, batch_size=32, shift=1):\n",
    "        \"\"\"\n",
    "        Pipeline untuk prediksi request count menggunakan LSTM dengan windowed dataset\n",
    "        \n",
    "        Args:\n",
    "            n_past: Jumlah time steps untuk input (lookback window)\n",
    "            n_future: Jumlah time steps untuk prediksi (forecast horizon) \n",
    "            batch_size: Batch size untuk training\n",
    "            shift: Shift untuk sliding window\n",
    "        \"\"\"\n",
    "        self.n_past = n_past\n",
    "        self.n_future = n_future\n",
    "        self.batch_size = batch_size\n",
    "        self.shift = shift\n",
    "        self.scaler = StandardScaler()\n",
    "        self.geo_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.feature_columns = []\n",
    "        \n",
    "    def windowed_dataset(self, series, batch_size, n_past=24, n_future=24, shift=1):\n",
    "        \"\"\"\n",
    "        Membuat windowed dataset untuk time series dengan TensorFlow\n",
    "        \n",
    "        Args:\n",
    "            series: Input time series data\n",
    "            batch_size: Batch size\n",
    "            n_past: Window size untuk input\n",
    "            n_future: Window size untuk output\n",
    "            shift: Shift untuk sliding window\n",
    "        \n",
    "        Returns:\n",
    "            tf.data.Dataset: Windowed dataset\n",
    "        \"\"\"\n",
    "        # Convert to TensorFlow dataset\n",
    "        ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "        \n",
    "        # Create windowed dataset\n",
    "        ds = ds.window(n_past + n_future, shift=shift, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
    "        ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
    "        \n",
    "        return ds.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    def load_and_preprocess_data(self, csv_file_path):\n",
    "        \"\"\"\n",
    "        Load dan preprocess data CSV untuk time series\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Convert datetime\n",
    "        df['request_date'] = pd.to_datetime(df['request_date'])\n",
    "        df['time_slot'] = pd.to_datetime(df['time_slot'], format='%H:%M-%H:%M', errors='coerce').dt.hour\n",
    "        \n",
    "        # Create datetime index combining date and hour\n",
    "        df['datetime'] = df['request_date'] + pd.to_timedelta(df['time_slot'], unit='h')\n",
    "        \n",
    "        # Sort by datetime and geo hash\n",
    "        df = df.sort_values(['origin_geo_hash', 'datetime'])\n",
    "        \n",
    "        # Feature engineering\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "        df['day_of_month'] = df['datetime'].dt.day\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "        df['is_peak_hour'] = ((df['hour'] >= 7) & (df['hour'] <= 9) | \n",
    "                              (df['hour'] >= 17) & (df['hour'] <= 19)).astype(int)\n",
    "        \n",
    "        # Encode geo hash\n",
    "        df['geo_encoded'] = self.geo_encoder.fit_transform(df['origin_geo_hash'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_multivariate_series(self, data):\n",
    "        \"\"\"\n",
    "        Membuat multivariate time series dari data\n",
    "        \"\"\"\n",
    "        series_data = []\n",
    "        \n",
    "        # Group by geo hash untuk menjaga continuity per lokasi\n",
    "        for geo_hash in data['origin_geo_hash'].unique():\n",
    "            geo_data = data[data['origin_geo_hash'] == geo_hash].copy()\n",
    "            geo_data = geo_data.sort_values('datetime')\n",
    "            \n",
    "            # Create complete datetime range untuk fill missing values\n",
    "            full_range = pd.date_range(\n",
    "                start=geo_data['datetime'].min(),\n",
    "                end=geo_data['datetime'].max(),\n",
    "                freq='H'  # Hourly frequency\n",
    "            )\n",
    "            \n",
    "            # Reindex untuk fill missing timestamps\n",
    "            geo_data = geo_data.set_index('datetime').reindex(full_range)\n",
    "            \n",
    "            # Forward fill missing values\n",
    "            geo_data = geo_data.fillna(method='ffill').fillna(method='bfill')\n",
    "            geo_data = geo_data.fillna(0)  # Fill remaining NaN with 0\n",
    "            \n",
    "            # Features untuk model (normalize target variable)\n",
    "            features = [\n",
    "                'request_count', 'hour', 'day_of_week', \n",
    "                'day_of_month', 'month', 'is_weekend', \n",
    "                'is_peak_hour', 'geo_encoded'\n",
    "            ]\n",
    "            \n",
    "            # Hanya ambil data yang cukup panjang untuk windowing\n",
    "            if len(geo_data) >= (self.n_past + self.n_future):\n",
    "                series_data.append(geo_data[features].values)\n",
    "        \n",
    "        # Gabungkan semua series\n",
    "        if series_data:\n",
    "            combined_series = np.concatenate(series_data, axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"No sufficient data for windowing\")\n",
    "        \n",
    "        return combined_series\n",
    "    \n",
    "    def prepare_data(self, csv_file_path, train_split=0.7, validation_split=0.2):\n",
    "        \"\"\"\n",
    "        Prepare data dengan temporal split (TIDAK menggunakan random split!)\n",
    "        \"\"\"\n",
    "        # Load dan preprocess\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        data = self.load_and_preprocess_data(csv_file_path)\n",
    "        \n",
    "        # Create multivariate series\n",
    "        print(\"Creating multivariate series...\")\n",
    "        series = self.create_multivariate_series(data)\n",
    "        \n",
    "        print(f\"Total time steps: {len(series)}\")\n",
    "        print(f\"Features: {series.shape[1]}\")\n",
    "        \n",
    "        # Normalize features\n",
    "        series_scaled = self.scaler.fit_transform(series)\n",
    "        \n",
    "        # TEMPORAL SPLIT (bukan random split!)\n",
    "        total_len = len(series_scaled)\n",
    "        train_size = int(total_len * train_split)\n",
    "        val_size = int(total_len * validation_split)\n",
    "        \n",
    "        # Split berdasarkan waktu\n",
    "        train_data = series_scaled[:train_size]\n",
    "        val_data = series_scaled[train_size:train_size + val_size]\n",
    "        test_data = series_scaled[train_size + val_size:]\n",
    "        \n",
    "        print(f\"Train size: {len(train_data)}\")\n",
    "        print(f\"Validation size: {len(val_data)}\")\n",
    "        print(f\"Test size: {len(test_data)}\")\n",
    "        \n",
    "        # Create windowed datasets\n",
    "        train_set = self.windowed_dataset(\n",
    "            train_data, self.batch_size, self.n_past, self.n_future, self.shift\n",
    "        )\n",
    "        \n",
    "        val_set = self.windowed_dataset(\n",
    "            val_data, self.batch_size, self.n_past, self.n_future, self.shift\n",
    "        )\n",
    "        \n",
    "        return train_set, val_set, test_data, series_scaled\n",
    "    \n",
    "    def build_model(self, n_features):\n",
    "        \"\"\"\n",
    "        Build LSTM model untuk multivariate time series\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Input(shape=(self.n_past, n_features)),\n",
    "            \n",
    "            # LSTM layers dengan return_sequences=True untuk stacked LSTM\n",
    "            LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "            \n",
    "            # Dense layers\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            # Output layer: predict multiple timesteps untuk multiple features\n",
    "            Dense(self.n_future * n_features, activation='linear'),\n",
    "            \n",
    "            # Reshape ke (batch_size, n_future, n_features)\n",
    "            tf.keras.layers.Reshape((self.n_future, n_features))\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, csv_file_path, epochs=100):\n",
    "        \"\"\"\n",
    "        Train model dengan proper time series validation\n",
    "        \"\"\"\n",
    "        # Prepare data dengan temporal split\n",
    "        train_set, val_set, test_data, scaled_series = self.prepare_data(csv_file_path)\n",
    "        \n",
    "        # Build model\n",
    "        n_features = scaled_series.shape[1]\n",
    "        self.model = self.build_model(n_features)\n",
    "        \n",
    "        print(\"Model Summary:\")\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                patience=15, \n",
    "                restore_best_weights=True,\n",
    "                monitor='val_loss'\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                factor=0.5, \n",
    "                patience=7, \n",
    "                min_lr=1e-7,\n",
    "                monitor='val_loss'\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training model...\")\n",
    "        history = self.model.fit(\n",
    "            train_set,\n",
    "            validation_data=val_set,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Store test data untuk evaluasi\n",
    "        self.test_data = test_data\n",
    "        self.scaled_series = scaled_series\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict_future(self, input_sequence, steps_ahead=24):\n",
    "        \"\"\"\n",
    "        Multi-step ahead prediction\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model belum di-train!\")\n",
    "        \n",
    "        predictions = []\n",
    "        current_sequence = input_sequence.copy()\n",
    "        \n",
    "        for _ in range(steps_ahead // self.n_future):\n",
    "            # Predict next n_future steps\n",
    "            pred = self.model.predict(\n",
    "                current_sequence.reshape(1, self.n_past, -1), \n",
    "                verbose=0\n",
    "            )[0]\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # Update sequence untuk prediksi berikutnya\n",
    "            # Slide window dan tambahkan prediksi\n",
    "            current_sequence = np.vstack([\n",
    "                current_sequence[self.n_future:],\n",
    "                pred\n",
    "            ])\n",
    "        \n",
    "        return np.concatenate(predictions, axis=0) if predictions else np.array([])\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluate model pada test set\n",
    "        \"\"\"\n",
    "        if self.test_data is None or len(self.test_data) < (self.n_past + self.n_future):\n",
    "            print(\"Insufficient test data for evaluation\")\n",
    "            return None\n",
    "        \n",
    "        # Create test dataset\n",
    "        test_set = self.windowed_dataset(\n",
    "            self.test_data, self.batch_size, self.n_past, self.n_future, self.shift\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_mae = self.model.evaluate(test_set, verbose=0)\n",
    "        print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "        print(f\"Test MAE: {test_mae:.4f}\")\n",
    "        \n",
    "        return test_loss, test_mae\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"\n",
    "        Plot training history\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss\n",
    "        ax1.plot(history.history['loss'], label='Training Loss')\n",
    "        ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        ax1.set_title('Model Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # MAE\n",
    "        ax2.plot(history.history['mae'], label='Training MAE')\n",
    "        ax2.plot(history.history['val_mae'], label='Validation MAE')\n",
    "        ax2.set_title('Model MAE')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('MAE')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca5b8226",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'studio_results_20250925_1926.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstudio_results_20250925_1926.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    935\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    936\u001b[39m     dialect,\n\u001b[32m    937\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    945\u001b[39m )\n\u001b[32m    946\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    608\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1447\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1704\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1716\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    862\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    864\u001b[39m             handle,\n\u001b[32m    865\u001b[39m             ioargs.mode,\n\u001b[32m    866\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    867\u001b[39m             errors=errors,\n\u001b[32m    868\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    869\u001b[39m         )\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    872\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'studio_results_20250925_1926.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('studio_results_20250925_1926.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c9f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with temporal split...\n",
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'studio_results_20250925_1926.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Train model dengan proper temporal split\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training with temporal split...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m history = \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstudio_results_20250925_1926.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[32m     22\u001b[39m predictor.plot_training_history(history)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 201\u001b[39m, in \u001b[36mTimeSeriesRequestPredictor.train\u001b[39m\u001b[34m(self, csv_file_path, epochs)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03mTrain model dengan proper time series validation\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Prepare data dengan temporal split\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m train_set, val_set, test_data, scaled_series = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# Build model\u001b[39;00m\n\u001b[32m    204\u001b[39m n_features = scaled_series.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mTimeSeriesRequestPredictor.prepare_data\u001b[39m\u001b[34m(self, csv_file_path, train_split, validation_split)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Load dan preprocess\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading and preprocessing data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Create multivariate series\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating multivariate series...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mTimeSeriesRequestPredictor.load_and_preprocess_data\u001b[39m\u001b[34m(self, csv_file_path)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03mLoad dan preprocess data CSV untuk time series\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Convert datetime\u001b[39;00m\n\u001b[32m     53\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mrequest_date\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mrequest_date\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    935\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    936\u001b[39m     dialect,\n\u001b[32m    937\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    945\u001b[39m )\n\u001b[32m    946\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    608\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1447\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1704\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1716\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    862\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    872\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'studio_results_20250925_1926.csv'"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "N_PAST = 24      # 24 hours lookback\n",
    "N_FUTURE = 24    # 24 hours forecast\n",
    "SHIFT = 1        # 1 hour shift\n",
    "\n",
    "# Contoh penggunaan\n",
    "if __name__ == \"__main__\":\n",
    "    # Inisialisasi predictor dengan windowed dataset\n",
    "    predictor = TimeSeriesRequestPredictor(\n",
    "        n_past=N_PAST,\n",
    "        n_future=N_FUTURE, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shift=SHIFT\n",
    "    )\n",
    "    \n",
    "    # Train model dengan proper temporal split\n",
    "    print(\"Starting training with temporal split...\")\n",
    "    history = predictor.train('studio_results_20250925_1926.csv', epochs=50)\n",
    "    \n",
    "    # Plot training history\n",
    "    predictor.plot_training_history(history)\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_results = predictor.evaluate_model()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Using temporal split instead of random split\")\n",
    "    print(f\"Window size: {N_PAST} -> {N_FUTURE}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
