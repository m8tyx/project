{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d402acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583e0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimeSeriesRequestPredictor:\n",
    "    def __init__(self, sequence_length=1, prediction_horizon=1):\n",
    "        \"\"\"\n",
    "        Pipeline prediksi request count dengan LSTM\n",
    "        Args:\n",
    "            sequence_length: panjang window historis (hari)\n",
    "            prediction_horizon: horizon prediksi (berapa hari ke depan)\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.scaler = StandardScaler()\n",
    "        self.geo_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def load_and_preprocess_data(self, csv_file_path):\n",
    "        \"\"\"\n",
    "        Load dan preprocess data CSV\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Pastikan request_date datetime\n",
    "        df[\"request_date\"] = pd.to_datetime(df[\"request_date\"], errors=\"coerce\")\n",
    "\n",
    "        # Ekstrak jam dari kolom time_slot (jika ada)\n",
    "        if \"time_slot\" in df.columns:\n",
    "            def extract_hour(val):\n",
    "                if pd.isna(val):\n",
    "                    return 0\n",
    "                if isinstance(val, str) and \"-\" in val:\n",
    "                    return int(val.split(\":\")[0])\n",
    "                try:\n",
    "                    return pd.to_datetime(val, errors=\"coerce\").hour\n",
    "                except Exception:\n",
    "                    return 0\n",
    "\n",
    "            df[\"time_slot\"] = df[\"time_slot\"].apply(extract_hour)\n",
    "        else:\n",
    "            df[\"time_slot\"] = 0\n",
    "\n",
    "        # Aggregate per hari per lokasi\n",
    "        daily_data = df.groupby([\"request_date\", \"origin_geo_hash\"]).agg({\n",
    "            \"request_count\": \"sum\",\n",
    "            \"time_slot\": \"count\"  # jumlah slot aktif\n",
    "        }).reset_index()\n",
    "\n",
    "        daily_data.rename(columns={\"time_slot\": \"active_slots\"}, inplace=True)\n",
    "\n",
    "        # Feature engineering\n",
    "        daily_data[\"day_of_week\"] = daily_data[\"request_date\"].dt.dayofweek\n",
    "        daily_data[\"day_of_month\"] = daily_data[\"request_date\"].dt.day\n",
    "        daily_data[\"month\"] = daily_data[\"request_date\"].dt.month\n",
    "        daily_data[\"is_weekend\"] = (daily_data[\"day_of_week\"] >= 5).astype(int)\n",
    "\n",
    "        # Encode geo hash\n",
    "        daily_data[\"geo_encoded\"] = self.geo_encoder.fit_transform(daily_data[\"origin_geo_hash\"])\n",
    "\n",
    "        # Sort by date and geo\n",
    "        daily_data = daily_data.sort_values([\"origin_geo_hash\", \"request_date\"])\n",
    "\n",
    "        return daily_data\n",
    "\n",
    "    def create_sequences(self, data):\n",
    "        \"\"\"\n",
    "        Buat sequences untuk LSTM\n",
    "        \"\"\"\n",
    "        sequences, targets = [], []\n",
    "        features = [\n",
    "            \"request_count\",\n",
    "            \"active_slots\",\n",
    "            \"day_of_week\",\n",
    "            \"day_of_month\",\n",
    "            \"month\",\n",
    "            \"is_weekend\",\n",
    "            \"geo_encoded\",\n",
    "        ]\n",
    "\n",
    "        for geo_hash in data[\"origin_geo_hash\"].unique():\n",
    "            geo_data = data[data[\"origin_geo_hash\"] == geo_hash].sort_values(\n",
    "                \"request_date\"\n",
    "            )\n",
    "\n",
    "            # Lengkapi missing dates\n",
    "            date_range = pd.date_range(\n",
    "                start=geo_data[\"request_date\"].min(),\n",
    "                end=geo_data[\"request_date\"].max(),\n",
    "                freq=\"D\",\n",
    "            )\n",
    "            geo_data = (\n",
    "                geo_data.set_index(\"request_date\")\n",
    "                .reindex(date_range, fill_value=0)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"request_date\"})\n",
    "            )\n",
    "\n",
    "            feature_data = geo_data[features].values\n",
    "\n",
    "            for i in range(len(feature_data) - self.sequence_length):\n",
    "                if (\n",
    "                    i + self.sequence_length + self.prediction_horizon\n",
    "                    <= len(feature_data)\n",
    "                ):\n",
    "                    seq = feature_data[i : i + self.sequence_length]\n",
    "                    target = feature_data[\n",
    "                        i + self.sequence_length : i + self.sequence_length\n",
    "                        + self.prediction_horizon,\n",
    "                        0,\n",
    "                    ]\n",
    "                    sequences.append(seq)\n",
    "                    targets.append(target)\n",
    "\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Build LSTM model\n",
    "        \"\"\"\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Input(shape=input_shape),\n",
    "                LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "                LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "                LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "                Dense(64, activation=\"relu\"),\n",
    "                Dropout(0.3),\n",
    "                Dense(32, activation=\"relu\"),\n",
    "                Dropout(0.2),\n",
    "                Dense(self.prediction_horizon, activation=\"linear\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        model.compile(optimizer=Adam(0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "        return model\n",
    "\n",
    "    def temporal_split(self, X, y, val_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Split data secara temporal (bukan random)\n",
    "        \"\"\"\n",
    "        split_index = int(len(X) * (1 - val_ratio))\n",
    "        X_train, X_val = X[:split_index], X[split_index:]\n",
    "        y_train, y_val = y[:split_index], y[split_index:]\n",
    "        return X_train, X_val, y_train, y_val\n",
    "\n",
    "    def train(self, csv_file_path, validation_split=0.2, epochs=50):\n",
    "        \"\"\"\n",
    "        Train model dengan temporal split\n",
    "        \"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        data = self.load_and_preprocess_data(csv_file_path)\n",
    "\n",
    "        print(\"Creating sequences...\")\n",
    "        X, y = self.create_sequences(data)\n",
    "\n",
    "        print(f\"Total sequences: {len(X)}\")\n",
    "        print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        # Scale\n",
    "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "        X_scaled = self.scaler.fit_transform(X_reshaped)\n",
    "        X_scaled = X_scaled.reshape(X.shape)\n",
    "\n",
    "        # Temporal split\n",
    "        X_train, X_val, y_train, y_val = self.temporal_split(X_scaled, y, validation_split)\n",
    "\n",
    "        # Build model\n",
    "        self.model = self.build_model((X.shape[1], X.shape[2]))\n",
    "        print(self.model.summary())\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6),\n",
    "        ]\n",
    "\n",
    "        history = self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        return data, history\n",
    "\n",
    "    def predict_next_day(self, data, geo_hash):\n",
    "        \"\"\"\n",
    "        Prediksi request count untuk besok di 1 lokasi (geo_hash)\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model belum di-train!\")\n",
    "\n",
    "        # Ambil data terakhir\n",
    "        geo_data = data[data[\"origin_geo_hash\"] == geo_hash].sort_values(\"request_date\")\n",
    "        features = [\n",
    "            \"request_count\", \"active_slots\", \"day_of_week\",\n",
    "            \"day_of_month\", \"month\", \"is_weekend\", \"geo_encoded\"\n",
    "        ]\n",
    "        \n",
    "        # Cek apakah ada cukup data untuk panjang urutan yang ditentukan\n",
    "        if len(geo_data) < self.sequence_length:\n",
    "            raise ValueError(f\"Tidak cukup data untuk geo_hash '{geo_hash}'. Dibutuhkan {self.sequence_length} hari, tapi hanya tersedia {len(geo_data)} hari.\")\n",
    "            \n",
    "        last_sequence = geo_data[features].tail(self.sequence_length).values\n",
    "\n",
    "        # Normalisasi: perbaikan di sini untuk mengatasi ValueError\n",
    "        seq_scaled = self.scaler.transform(last_sequence)\n",
    "        seq_scaled = seq_scaled.reshape(1, self.sequence_length, len(features))\n",
    "\n",
    "        # Prediksi\n",
    "        pred = self.model.predict(seq_scaled, verbose=0)[0]\n",
    "\n",
    "        # Tentukan tanggal besok\n",
    "        tomorrow = geo_data[\"request_date\"].max() + timedelta(days=1)\n",
    "\n",
    "        # Kembalikan sebagai tabel (DataFrame)\n",
    "        result_df = pd.DataFrame({\n",
    "            \"geo_hash\": [geo_hash],\n",
    "            \"date\": [tomorrow],\n",
    "            \"predicted_request_count\": [max(0, pred[0])] # Jaga jangan negatif\n",
    "        })\n",
    "        return result_df\n",
    "    \n",
    "    def predict_all_next_day(self, data):\n",
    "        \"\"\"\n",
    "        Prediksi request_count besok untuk semua lokasi (geo_hash)\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        tomorrow = data[\"request_date\"].max() + timedelta(days=1)\n",
    "\n",
    "        for geo_hash in data[\"origin_geo_hash\"].unique():\n",
    "            try:\n",
    "                pred_df = self.predict_next_day(data, geo_hash)\n",
    "                results.append(pred_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {geo_hash}, error: {e}\")\n",
    "\n",
    "        if results:\n",
    "            return pd.concat(results, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame(columns=[\"geo_hash\", \"date\", \"predicted_request_count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "505c9f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Creating sequences...\n",
      "Total sequences: 2064\n",
      "X shape: (2064, 7, 7), y shape: (2064, 1)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_9 (LSTM)               (None, 7, 128)            69632     \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 7, 64)             49408     \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135681 (530.00 KB)\n",
      "Trainable params: 135681 (530.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "52/52 [==============================] - 7s 20ms/step - loss: 159.5560 - mae: 8.8136 - val_loss: 237.4391 - val_mae: 9.8713 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 104.6042 - mae: 7.3420 - val_loss: 203.7354 - val_mae: 9.3606 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 100.7007 - mae: 7.0919 - val_loss: 190.0219 - val_mae: 9.5725 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 96.0731 - mae: 7.1237 - val_loss: 177.2720 - val_mae: 9.2799 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 97.6373 - mae: 7.1721 - val_loss: 186.0454 - val_mae: 8.6783 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 98.0279 - mae: 7.0765 - val_loss: 175.2835 - val_mae: 8.8888 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 94.5552 - mae: 7.0135 - val_loss: 186.6350 - val_mae: 8.5308 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 94.9623 - mae: 7.0260 - val_loss: 169.1765 - val_mae: 8.7367 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 95.5916 - mae: 6.9511 - val_loss: 165.8013 - val_mae: 8.7456 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 96.0864 - mae: 6.9957 - val_loss: 164.7170 - val_mae: 8.7034 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 93.3693 - mae: 7.0240 - val_loss: 163.3249 - val_mae: 8.4166 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 93.7073 - mae: 6.9182 - val_loss: 159.1787 - val_mae: 8.7166 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 95.1358 - mae: 7.0154 - val_loss: 164.4078 - val_mae: 8.6075 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 93.4209 - mae: 6.9341 - val_loss: 161.8498 - val_mae: 8.3521 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 93.1208 - mae: 6.8880 - val_loss: 154.5504 - val_mae: 8.4543 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 94.9366 - mae: 7.0259 - val_loss: 155.8111 - val_mae: 8.3826 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 91.6252 - mae: 6.8976 - val_loss: 148.3371 - val_mae: 8.3613 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 93.5436 - mae: 6.9064 - val_loss: 154.4921 - val_mae: 8.1616 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 96.5904 - mae: 7.0131 - val_loss: 150.6354 - val_mae: 8.6682 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 92.0712 - mae: 6.8932 - val_loss: 145.7096 - val_mae: 8.2874 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 94.0578 - mae: 6.9262 - val_loss: 150.1718 - val_mae: 8.6168 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 92.4772 - mae: 6.8998 - val_loss: 146.5647 - val_mae: 8.2998 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 91.7221 - mae: 6.8452 - val_loss: 147.0138 - val_mae: 8.3588 - lr: 0.0010\n",
      "Epoch 24/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 92.5745 - mae: 6.9372 - val_loss: 144.9559 - val_mae: 8.4742 - lr: 0.0010\n",
      "Epoch 25/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 90.9862 - mae: 6.8268 - val_loss: 144.5586 - val_mae: 8.2080 - lr: 0.0010\n",
      "Epoch 26/30\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 91.1553 - mae: 6.8909 - val_loss: 143.8468 - val_mae: 8.1842 - lr: 0.0010\n",
      "Epoch 27/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 94.5867 - mae: 6.8804 - val_loss: 147.9378 - val_mae: 8.2968 - lr: 0.0010\n",
      "Epoch 28/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 92.4266 - mae: 6.9163 - val_loss: 143.1158 - val_mae: 8.1949 - lr: 0.0010\n",
      "Epoch 29/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 90.9745 - mae: 6.8541 - val_loss: 146.8417 - val_mae: 8.2760 - lr: 0.0010\n",
      "Epoch 30/30\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 89.2438 - mae: 6.8820 - val_loss: 142.2733 - val_mae: 8.2922 - lr: 0.0010\n",
      "\n",
      "--- Prediksi untuk semua lokasi di hari berikutnya ---\n",
      "Skipping qqgeut, error: Tidak cukup data untuk geo_hash 'qqgeut'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqggyc, error: Tidak cukup data untuk geo_hash 'qqggyc'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqgu4s, error: Tidak cukup data untuk geo_hash 'qqgu4s'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqgudx, error: Tidak cukup data untuk geo_hash 'qqgudx'. Dibutuhkan 7 hari, tapi hanya tersedia 3 hari.\n",
      "Skipping qqgug5, error: Tidak cukup data untuk geo_hash 'qqgug5'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguk9, error: Tidak cukup data untuk geo_hash 'qqguk9'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqgumg, error: Tidak cukup data untuk geo_hash 'qqgumg'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqgunw, error: Tidak cukup data untuk geo_hash 'qqgunw'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqguqp, error: Tidak cukup data untuk geo_hash 'qqguqp'. Dibutuhkan 7 hari, tapi hanya tersedia 3 hari.\n",
      "Skipping qqgurz, error: Tidak cukup data untuk geo_hash 'qqgurz'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqguuc, error: Tidak cukup data untuk geo_hash 'qqguuc'. Dibutuhkan 7 hari, tapi hanya tersedia 5 hari.\n",
      "Skipping qqguw9, error: Tidak cukup data untuk geo_hash 'qqguw9'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguwg, error: Tidak cukup data untuk geo_hash 'qqguwg'. Dibutuhkan 7 hari, tapi hanya tersedia 3 hari.\n",
      "Skipping qqguwr, error: Tidak cukup data untuk geo_hash 'qqguwr'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguws, error: Tidak cukup data untuk geo_hash 'qqguws'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguwu, error: Tidak cukup data untuk geo_hash 'qqguwu'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqguww, error: Tidak cukup data untuk geo_hash 'qqguww'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping qqguwz, error: Tidak cukup data untuk geo_hash 'qqguwz'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqgux6, error: Tidak cukup data untuk geo_hash 'qqgux6'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping qqguxh, error: Tidak cukup data untuk geo_hash 'qqguxh'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguxr, error: Tidak cukup data untuk geo_hash 'qqguxr'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguyd, error: Tidak cukup data untuk geo_hash 'qqguyd'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqguyu, error: Tidak cukup data untuk geo_hash 'qqguyu'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqguz0, error: Tidak cukup data untuk geo_hash 'qqguz0'. Dibutuhkan 7 hari, tapi hanya tersedia 5 hari.\n",
      "Skipping qqguzb, error: Tidak cukup data untuk geo_hash 'qqguzb'. Dibutuhkan 7 hari, tapi hanya tersedia 5 hari.\n",
      "Skipping qqguzg, error: Tidak cukup data untuk geo_hash 'qqguzg'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqguzv, error: Tidak cukup data untuk geo_hash 'qqguzv'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqgvhz, error: Tidak cukup data untuk geo_hash 'qqgvhz'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqgvnr, error: Tidak cukup data untuk geo_hash 'qqgvnr'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqgvp3, error: Tidak cukup data untuk geo_hash 'qqgvp3'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqtn3d, error: Tidak cukup data untuk geo_hash 'qqtn3d'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqttv8, error: Tidak cukup data untuk geo_hash 'qqttv8'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqu486, error: Tidak cukup data untuk geo_hash 'qqu486'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping qqu5bh, error: Tidak cukup data untuk geo_hash 'qqu5bh'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping qqu8b6, error: Tidak cukup data untuk geo_hash 'qqu8b6'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qquh4v, error: Tidak cukup data untuk geo_hash 'qquh4v'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qquh8r, error: Tidak cukup data untuk geo_hash 'qquh8r'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qquj03, error: Tidak cukup data untuk geo_hash 'qquj03'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qquj0x, error: Tidak cukup data untuk geo_hash 'qquj0x'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqvbu2, error: Tidak cukup data untuk geo_hash 'qqvbu2'. Dibutuhkan 7 hari, tapi hanya tersedia 2 hari.\n",
      "Skipping qqww69, error: Tidak cukup data untuk geo_hash 'qqww69'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqwx2g, error: Tidak cukup data untuk geo_hash 'qqwx2g'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqy08y, error: Tidak cukup data untuk geo_hash 'qqy08y'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qqy2n8, error: Tidak cukup data untuk geo_hash 'qqy2n8'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qw3ytk, error: Tidak cukup data untuk geo_hash 'qw3ytk'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qw3yug, error: Tidak cukup data untuk geo_hash 'qw3yug'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qw3ywj, error: Tidak cukup data untuk geo_hash 'qw3ywj'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qw84f6, error: Tidak cukup data untuk geo_hash 'qw84f6'. Dibutuhkan 7 hari, tapi hanya tersedia 1 hari.\n",
      "Skipping qxg0gp, error: Tidak cukup data untuk geo_hash 'qxg0gp'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping qxhfwj, error: Tidak cukup data untuk geo_hash 'qxhfwj'. Dibutuhkan 7 hari, tapi hanya tersedia 6 hari.\n",
      "Skipping w21yq4, error: Tidak cukup data untuk geo_hash 'w21yq4'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping w24m4p, error: Tidak cukup data untuk geo_hash 'w24m4p'. Dibutuhkan 7 hari, tapi hanya tersedia 4 hari.\n",
      "Skipping w8589q, error: Tidak cukup data untuk geo_hash 'w8589q'. Dibutuhkan 7 hari, tapi hanya tersedia 5 hari.\n",
      "Skipping w858c1, error: Tidak cukup data untuk geo_hash 'w858c1'. Dibutuhkan 7 hari, tapi hanya tersedia 3 hari.\n",
      "    geo_hash       date  predicted_request_count\n",
      "0     qpz6e8 2025-09-25                 6.880337\n",
      "1     qqgb0x 2025-09-26                11.248613\n",
      "2     qqgfmy 2025-09-26                 8.974757\n",
      "3     qqgfps 2025-09-26                 6.530941\n",
      "4     qqgfqd 2025-09-26                 7.598875\n",
      "..       ...        ...                      ...\n",
      "281   w0whm4 2025-09-26                16.157093\n",
      "282   w0wht9 2025-09-26                22.184532\n",
      "283   w204gg 2025-09-26                 5.841674\n",
      "284   w204ut 2025-09-24                 8.934999\n",
      "285   w21yrk 2025-09-26                13.374604\n",
      "\n",
      "[286 rows x 3 columns]\n",
      "\n",
      "--- Prediksi untuk satu lokasi (misal 'qqgumz') ---\n",
      "  geo_hash       date  predicted_request_count\n",
      "0   qqgumz 2025-09-26                10.734144\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Inisialisasi model dengan sequence_length yang lebih optimal\n",
    "    # Menggunakan 7 hari untuk menangkap pola mingguan\n",
    "    predictor = TimeSeriesRequestPredictor(sequence_length=7, prediction_horizon=1)\n",
    "\n",
    "    # Latih model dengan data CSV. Pastikan file berada di direktori yang sama.\n",
    "    try:\n",
    "        data, history = predictor.train(\n",
    "            \"studio_results_20250925_1926.csv\",\n",
    "            epochs=30\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File 'studio_results_20250925_1926.csv' tidak ditemukan.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan saat melatih model: {e}\")\n",
    "\n",
    "    # Jika pelatihan berhasil, lanjutkan ke prediksi\n",
    "    if predictor.model:\n",
    "        # Prediksi semua lokasi untuk hari berikutnya\n",
    "        print(\"\\n--- Prediksi untuk semua lokasi di hari berikutnya ---\")\n",
    "        predictions_df = predictor.predict_all_next_day(data)\n",
    "        print(predictions_df) # Tampilkan beberapa baris pertama\n",
    "\n",
    "        # Contoh prediksi dan plot untuk satu lokasi\n",
    "        try:\n",
    "            print(\"\\n--- Prediksi untuk satu lokasi (misal 'qqgumz') ---\")\n",
    "            pred_table = predictor.predict_next_day(data, \"qqgumz\")\n",
    "            print(pred_table)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536c4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv(\"predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
