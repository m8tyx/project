{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d402acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "583e0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesRequestPredictor:\n",
    "    def __init__(self, sequence_length=7, prediction_horizon=1):\n",
    "        \"\"\"\n",
    "        Pipeline prediksi request count dengan LSTM\n",
    "        Args:\n",
    "            sequence_length: panjang window historis (hari)\n",
    "            prediction_horizon: horizon prediksi (berapa hari ke depan)\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.scaler = StandardScaler()\n",
    "        self.geo_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.last_trained_date = None   # simpan tanggal terakhir data training\n",
    "\n",
    "    # -------------------- LOAD & PREPROCESS --------------------\n",
    "    def load_and_preprocess_data(self, csv_file_path):\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Pastikan request_date datetime\n",
    "        df[\"request_date\"] = pd.to_datetime(df[\"request_date\"], errors=\"coerce\")\n",
    "\n",
    "        # Ekstrak jam\n",
    "        if \"time_slot\" in df.columns:\n",
    "            def extract_hour(val):\n",
    "                if pd.isna(val):\n",
    "                    return -1   # NaN -> dummy\n",
    "                if isinstance(val, str) and \"-\" in val:\n",
    "                    return int(val.split(\":\")[0])\n",
    "                try:\n",
    "                    return pd.to_datetime(val, errors=\"coerce\").hour\n",
    "                except Exception:\n",
    "                    return -1\n",
    "            df[\"time_slot\"] = df[\"time_slot\"].apply(extract_hour)\n",
    "        else:\n",
    "            df[\"time_slot\"] = -1\n",
    "\n",
    "        # Dummy flag\n",
    "        df[\"is_missing_slot\"] = (df[\"time_slot\"] == -1).astype(int)\n",
    "\n",
    "        # Aggregate per hari per lokasi\n",
    "        daily_data = df.groupby([\"request_date\", \"origin_geo_hash\"]).agg({\n",
    "            \"request_count\": \"sum\",\n",
    "            \"time_slot\": \"count\"\n",
    "        }).reset_index()\n",
    "\n",
    "        daily_data.rename(columns={\"time_slot\": \"active_slots\"}, inplace=True)\n",
    "\n",
    "        # Feature engineering\n",
    "        daily_data[\"day_of_week\"] = daily_data[\"request_date\"].dt.dayofweek\n",
    "        daily_data[\"day_of_month\"] = daily_data[\"request_date\"].dt.day\n",
    "        daily_data[\"month\"] = daily_data[\"request_date\"].dt.month\n",
    "        daily_data[\"is_weekend\"] = (daily_data[\"day_of_week\"] >= 5).astype(int)\n",
    "\n",
    "        # Encode geo\n",
    "        daily_data[\"geo_encoded\"] = self.geo_encoder.fit_transform(daily_data[\"origin_geo_hash\"])\n",
    "\n",
    "        # Sort\n",
    "        daily_data = daily_data.sort_values([\"origin_geo_hash\", \"request_date\"])\n",
    "        return daily_data\n",
    "\n",
    "    # -------------------- SEQUENCE CREATION --------------------\n",
    "    def create_sequences(self, data):\n",
    "        sequences, targets = [], []\n",
    "        features = [\n",
    "            \"request_count\",\n",
    "            \"active_slots\",\n",
    "            \"day_of_week\",\n",
    "            \"day_of_month\",\n",
    "            \"month\",\n",
    "            \"is_weekend\",\n",
    "            \"geo_encoded\",\n",
    "        ]\n",
    "\n",
    "        for geo_hash in data[\"origin_geo_hash\"].unique():\n",
    "            geo_data = data[data[\"origin_geo_hash\"] == geo_hash].sort_values(\"request_date\")\n",
    "\n",
    "            # Lengkapi missing date\n",
    "            date_range = pd.date_range(\n",
    "                start=geo_data[\"request_date\"].min(),\n",
    "                end=geo_data[\"request_date\"].max(),\n",
    "                freq=\"D\"\n",
    "            )\n",
    "            geo_data = (\n",
    "                geo_data.set_index(\"request_date\")\n",
    "                .reindex(date_range, fill_value=0)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"request_date\"})\n",
    "            )\n",
    "\n",
    "            feature_data = geo_data[features].values\n",
    "\n",
    "            for i in range(len(feature_data) - self.sequence_length):\n",
    "                if i + self.sequence_length + self.prediction_horizon <= len(feature_data):\n",
    "                    seq = feature_data[i:i + self.sequence_length]\n",
    "                    target = feature_data[\n",
    "                        i + self.sequence_length:i + self.sequence_length + self.prediction_horizon, 0\n",
    "                    ]\n",
    "                    sequences.append(seq)\n",
    "                    targets.append(target)\n",
    "\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    # -------------------- MODEL --------------------\n",
    "    def build_model(self, input_shape):\n",
    "        model = Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "            LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(self.prediction_horizon, activation=\"linear\"),\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "        return model\n",
    "\n",
    "    # -------------------- TRAIN --------------------\n",
    "    def temporal_split(self, X, y, val_ratio=0.2):\n",
    "        split_index = int(len(X) * (1 - val_ratio))\n",
    "        return X[:split_index], X[split_index:], y[:split_index], y[split_index:]\n",
    "\n",
    "    def train(self, csv_file_path, validation_split=0.2, epochs=50):\n",
    "        print(\"Loading data...\")\n",
    "        data = self.load_and_preprocess_data(csv_file_path)\n",
    "\n",
    "        print(\"Creating sequences...\")\n",
    "        X, y = self.create_sequences(data)\n",
    "\n",
    "        X_scaled = self.scaler.fit_transform(X.reshape(-1, X.shape[-1]))\n",
    "        X_scaled = X_scaled.reshape(X.shape)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = self.temporal_split(X_scaled, y, validation_split)\n",
    "\n",
    "        self.model = self.build_model((X.shape[1], X.shape[2]))\n",
    "        print(self.model.summary())\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6),\n",
    "        ]\n",
    "\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        self.last_trained_date = data[\"request_date\"].max()\n",
    "        return data, history\n",
    "\n",
    "    # -------------------- UPDATE (FINE TUNE) --------------------\n",
    "    def update_with_new_data(self, new_csv_file, epochs=10):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model belum di-train!\")\n",
    "\n",
    "        new_data = self.load_and_preprocess_data(new_csv_file)\n",
    "        X_new, y_new = self.create_sequences(new_data)\n",
    "\n",
    "        X_new_scaled = self.scaler.transform(X_new.reshape(-1, X_new.shape[-1]))\n",
    "        X_new_scaled = X_new_scaled.reshape(X_new.shape)\n",
    "\n",
    "        print(f\"Fine-tuning model dengan {len(X_new_scaled)} sequences dari data baru...\")\n",
    "        history = self.model.fit(\n",
    "            X_new_scaled, y_new,\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            verbose=1\n",
    "        )\n",
    "        self.last_trained_date = new_data[\"request_date\"].max()\n",
    "        return new_data, history\n",
    "\n",
    "    # -------------------- PREDICTION --------------------\n",
    "    def predict_next_day(self, data, geo_hash):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model belum di-train!\")\n",
    "\n",
    "        geo_data = data[data[\"origin_geo_hash\"] == geo_hash].sort_values(\"request_date\")\n",
    "        features = [\n",
    "            \"request_count\", \"active_slots\", \"day_of_week\",\n",
    "            \"day_of_month\", \"month\", \"is_weekend\", \"geo_encoded\"\n",
    "        ]\n",
    "\n",
    "        if len(geo_data) < self.sequence_length:\n",
    "            raise ValueError(f\"Data geo_hash '{geo_hash}' kurang dari {self.sequence_length} hari.\")\n",
    "\n",
    "        last_sequence = geo_data[features].tail(self.sequence_length).values\n",
    "        seq_scaled = self.scaler.transform(last_sequence)\n",
    "        seq_scaled = seq_scaled.reshape(1, self.sequence_length, len(features))\n",
    "\n",
    "        pred = self.model.predict(seq_scaled, verbose=0)[0]\n",
    "        tomorrow = geo_data[\"request_date\"].max() + timedelta(days=1)\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            \"geo_hash\": [geo_hash],\n",
    "            \"date\": [tomorrow],\n",
    "            \"predicted_request_count\": [max(0, pred[0])]\n",
    "        })\n",
    "\n",
    "    def predict_all_next_day(self, data):\n",
    "        results = []\n",
    "        for geo_hash in data[\"origin_geo_hash\"].unique():\n",
    "            try:\n",
    "                pred_df = self.predict_next_day(data, geo_hash)\n",
    "                results.append(pred_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {geo_hash}, error: {e}\")\n",
    "\n",
    "        return pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "\n",
    "    # -------------------- EVALUATION --------------------\n",
    "    def evaluate_yesterday_prediction(self, data, yesterday_predictions):\n",
    "        actuals = data[data[\"request_date\"].isin(yesterday_predictions[\"date\"])][\n",
    "            [\"origin_geo_hash\", \"request_date\", \"request_count\"]\n",
    "        ]\n",
    "        actuals = actuals.rename(columns={\n",
    "            \"origin_geo_hash\": \"geo_hash\",\n",
    "            \"request_date\": \"date\",\n",
    "            \"request_count\": \"actual_request_count\"\n",
    "        })\n",
    "\n",
    "        merged = pd.merge(yesterday_predictions, actuals, on=[\"geo_hash\", \"date\"], how=\"inner\")\n",
    "        if merged.empty:\n",
    "            print(\"⚠️ Tidak ada data aktual untuk evaluasi.\")\n",
    "            return None\n",
    "\n",
    "        mae = mean_absolute_error(merged[\"actual_request_count\"], merged[\"predicted_request_count\"])\n",
    "        rmse = mean_squared_error(merged[\"actual_request_count\"], merged[\"predicted_request_count\"], squared=False)\n",
    "\n",
    "        print(f\"Evaluasi {merged['date'].iloc[0].date()}: MAE={mae:.2f}, RMSE={rmse:.2f}\")\n",
    "        return merged, {\"MAE\": mae, \"RMSE\": rmse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "505c9f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train_awal.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m predictor = TimeSeriesRequestPredictor(sequence_length=\u001b[32m7\u001b[39m, prediction_horizon=\u001b[32m1\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Train model dengan dataset awal\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m data, history = \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/train_awal.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Prediksi besok untuk semua lokasi\u001b[39;00m\n\u001b[32m     10\u001b[39m predictions_dayN = predictor.predict_all_next_day(data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 127\u001b[39m, in \u001b[36mTimeSeriesRequestPredictor.train\u001b[39m\u001b[34m(self, csv_file_path, validation_split, epochs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_file_path, validation_split=\u001b[32m0.2\u001b[39m, epochs=\u001b[32m50\u001b[39m):\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating sequences...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    130\u001b[39m     X, y = \u001b[38;5;28mself\u001b[39m.create_sequences(data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mTimeSeriesRequestPredictor.load_and_preprocess_data\u001b[39m\u001b[34m(self, csv_file_path)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_preprocess_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_file_path):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Pastikan request_date datetime\u001b[39;00m\n\u001b[32m     21\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mrequest_date\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(df[\u001b[33m\"\u001b[39m\u001b[33mrequest_date\u001b[39m\u001b[33m\"\u001b[39m], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    935\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    936\u001b[39m     dialect,\n\u001b[32m    937\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    945\u001b[39m )\n\u001b[32m    946\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    608\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1447\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1704\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1716\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\Documents\\data\\proyek\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    862\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    864\u001b[39m             handle,\n\u001b[32m    865\u001b[39m             ioargs.mode,\n\u001b[32m    866\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    867\u001b[39m             errors=errors,\n\u001b[32m    868\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    869\u001b[39m         )\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    872\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/train_awal.csv'"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. TRAIN PERTAMA\n",
    "# ===============================\n",
    "predictor = TimeSeriesRequestPredictor(sequence_length=7, prediction_horizon=1)\n",
    "\n",
    "# Train model dengan dataset awal\n",
    "data, history = predictor.train(\"data/train_awal.csv\", epochs=30)\n",
    "\n",
    "# Prediksi besok untuk semua lokasi\n",
    "predictions_dayN = predictor.predict_all_next_day(data)\n",
    "print(\"Prediksi besok (training awal):\")\n",
    "print(predictions_dayN.head())\n",
    "\n",
    "# Simpan prediksi ke file (misalnya buat evaluasi besok)\n",
    "predictions_dayN.to_csv(\"prediksi_hariN.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
